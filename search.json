{"config":{"separator":"[\\s\\-_,:!=\\[\\]()\\\\\"`/]+|\\.(?!\\d)"},"items":[{"location":"","level":1,"title":"Timefence — Temporal correctness for ML training data","text":"<ul> <li> <p> Guaranteed Correctness</p> <p>Enforce <code>feature_time &lt; label_time</code> for every row. Embargo, staleness, and lookback — all configurable.</p> <p> Learn more</p> </li> <li> <p> Lightning Fast</p> <p>Built on DuckDB. Process millions of rows locally in seconds. No Spark cluster or cloud infrastructure needed.</p> <p> See benchmarks</p> </li> <li> <p> Audit Any Dataset</p> <p>Don't rebuild — just audit. Point at any existing training set and get a full leakage report instantly.</p> <p> Audit guide</p> </li> <li> <p> CI/CD Ready</p> <p><code>--strict</code> exits code 1 on leakage. Add temporal correctness to your pipeline in one line.</p> <p> CI guide</p> </li> </ul> How it works <p>Define your data, and Timefence handles point-in-time correctness for every row.</p> 1 Define Sources <p>Tell Timefence where your raw data lives and which columns represent time and entities.</p> <pre><code>src = timefence.Source(\n  path=\"txns.parquet\",\n  keys=[\"user_id\"],\n  timestamp=\"ts\",\n)</code></pre> 2 Build Clean Data <p>For every label, Timefence finds the most recent feature value strictly before the label timestamp — respecting embargo, lookback, and staleness.</p> <pre><code>df = timefence.build(\n  features=features,\n  labels=labels,\n  output=\"train.parquet\",\n)</code></pre> 3 Verify &amp; Ship <p>Audit the output to confirm zero leakage. A build manifest records exactly what happened.</p> <pre><code>report = timefence.audit(\n  \"train.parquet\",\n  features=features,\n  labels=labels,\n)\nreport.assert_clean()</code></pre> 1M+ rows in seconds Zero infrastructure needed 1 line to add to CI/CD 100% row-level guarantees","path":["Timefence — Temporal correctness for ML training data"],"tags":[]},{"location":"#your-features-know-the-futureyour-model-shouldnt","level":2,"title":"Your features know the future.Your model shouldn't.","text":"<p> When you join features to labels, future data leaks in — no error, no warning. Timefence finds it, fixes it, and proves every row is clean. </p> pip install timefence Copied! <p>Get Started View on GitHub</p>","path":["Timefence — Temporal correctness for ML training data"],"tags":[]},{"location":"#see-it-in-action","level":2,"title":"See it in action","text":"<p>Point Timefence at any training set. If future data leaked in, it finds it.</p> timefence <pre><code>$ timefence audit train_fraud_v3.parquet\n\nTEMPORAL AUDIT REPORT\nScanned 1,247,392 rows across 6 features\n\nWARNING  LEAKAGE DETECTED in 2 of 6 features\n\n  LEAK  merchant_risk_score\n        41,580 rows (3.3%) — future data\n        Severity: MEDIUM\n\n  LEAK  rolling_txn_count_7d\n        98,423 rows (7.9%) — future data\n        Severity: HIGH\n\n  OK    account_age_days — clean\n  OK    avg_txn_amount_30d — clean\n  OK    device_fingerprint_count — clean\n  OK    customer_tenure_months — clean\n\n$ timefence build -o train_fraud_v3_clean.parquet\n$ timefence audit train_fraud_v3_clean.parquet\nALL CLEAN — no temporal leakage detected</code></pre> <p>Try the quickstart </p>","path":["Timefence — Temporal correctness for ML training data"],"tags":[]},{"location":"#ready-to-find-out-if-your-training-data-is-clean","level":2,"title":"Ready to find out if your training data is clean?","text":"<p>Get Started in 60 Seconds Read the Docs</p>","path":["Timefence — Temporal correctness for ML training data"],"tags":[]},{"location":"api/","level":1,"title":"API Reference","text":"<p>Timefence's Python API is organized around five core objects and four functions.</p>","path":["API Reference"],"tags":[]},{"location":"api/#core-objects","level":2,"title":"Core Objects","text":"Class Purpose <code>Source</code> A historical data source (Parquet, CSV, DataFrame) <code>Feature</code> A named signal derived from a Source <code>Labels</code> Prediction target definition <code>FeatureSet</code> Group features for reuse <code>Store</code> Build tracking and caching","path":["API Reference"],"tags":[]},{"location":"api/#functions","level":2,"title":"Functions","text":"Function Purpose <code>build()</code> Construct a point-in-time correct training dataset <code>audit()</code> Scan a dataset for temporal leakage <code>explain()</code> Preview join logic without executing <code>diff()</code> Compare two datasets for changes","path":["API Reference"],"tags":[]},{"location":"api/#quick-example","level":2,"title":"Quick example","text":"<pre><code>import timefence\n\nsource = timefence.Source(path=\"data/users.parquet\", keys=[\"user_id\"], timestamp=\"updated_at\")\nfeature = timefence.Feature(source=source, columns=[\"country\"])\nlabels = timefence.Labels(path=\"data/labels.parquet\", keys=[\"user_id\"], label_time=\"label_time\", target=\"churned\")\n\nresult = timefence.build(labels=labels, features=[feature], output=\"train.parquet\")\n</code></pre>","path":["API Reference"],"tags":[]},{"location":"api/audit/","level":1,"title":"audit()","text":"<p>Scan a dataset for temporal leakage. Two modes are available.</p>","path":["API Reference","audit()"],"tags":[]},{"location":"api/audit/#timefence.audit","level":2,"title":"<code>audit(data, features=None, *, keys=None, label_time=None, feature_time_columns=None, max_lookback=DEFAULT_MAX_LOOKBACK, max_staleness=None, join='strict')</code>","text":"<p>Audit a dataset for temporal leakage.</p> <p>Two modes: 1. Rebuild-and-compare: provide features, keys, label_time. 2. Temporal check: provide feature_time_columns.</p> Source code in <code>src/timefence/engine.py</code> <pre><code>def audit(\n    data: str | Path | Any,\n    features: Sequence[Feature | FeatureSet] | None = None,\n    *,\n    keys: str | list[str] | None = None,\n    label_time: str | None = None,\n    feature_time_columns: dict[str, str] | None = None,\n    max_lookback: str | timedelta = DEFAULT_MAX_LOOKBACK,\n    max_staleness: str | timedelta | None = None,\n    join: str = \"strict\",\n) -&gt; AuditReport:\n    \"\"\"Audit a dataset for temporal leakage.\n\n    Two modes:\n    1. Rebuild-and-compare: provide features, keys, label_time.\n    2. Temporal check: provide feature_time_columns.\n    \"\"\"\n    if feature_time_columns is not None:\n        return _audit_temporal(data, feature_time_columns, label_time or \"label_time\")\n\n    if features is None:\n        raise TimefenceValidationError(\n            \"audit() requires either 'features' (for rebuild-and-compare) \"\n            \"or 'feature_time_columns' (for temporal check).\"\n        )\n    if keys is None or label_time is None:\n        raise TimefenceValidationError(\n            \"audit() in rebuild-and-compare mode requires 'keys' and 'label_time'.\"\n        )\n\n    return _audit_rebuild(\n        data,\n        features,\n        keys,\n        label_time,\n        max_lookback=max_lookback,\n        max_staleness=max_staleness,\n        join=join,\n    )\n</code></pre>","path":["API Reference","audit()"],"tags":[]},{"location":"api/audit/#mode-1-rebuild-and-compare-full-audit","level":2,"title":"Mode 1: Rebuild-and-compare (full audit)","text":"<pre><code>report = timefence.audit(\n    data=\"data/train.parquet\",\n    features=[rolling_spend, user_country],\n    keys=[\"user_id\"],\n    label_time=\"label_time\",\n    max_lookback=\"365d\",\n    max_staleness=None,\n    join=\"strict\",\n)\n</code></pre>","path":["API Reference","audit()"],"tags":[]},{"location":"api/audit/#mode-2-temporal-check-lightweight","level":2,"title":"Mode 2: Temporal check (lightweight)","text":"<pre><code>report = timefence.audit(\n    data=\"data/train.parquet\",\n    feature_time_columns={\n        \"spend_30d\": \"spend_computed_at\",\n        \"country\": \"country_updated_at\",\n    },\n    label_time=\"label_time\",\n)\n</code></pre>","path":["API Reference","audit()"],"tags":[]},{"location":"api/audit/#returns-auditreport","level":2,"title":"Returns: AuditReport","text":"Attribute Type Description <code>.has_leakage</code> <code>bool</code> <code>True</code> if any feature is leaky. <code>.clean_features</code> <code>list[str]</code> Feature names with no leakage. <code>.leaky_features</code> <code>list[str]</code> Feature names with leakage. <code>.total_rows</code> <code>int</code> Total rows scanned. <code>.mode</code> <code>str</code> <code>\"rebuild\"</code> or <code>\"temporal\"</code>. <code>[name]</code> <code>FeatureAuditDetail</code> Per-feature detail via <code>report[\"feature_name\"]</code>. <code>.assert_clean()</code> <code>None</code> Raises <code>TimefenceLeakageError</code> if leakage found. <code>.to_html(path)</code> <code>None</code> Export HTML report. <code>.to_json(path)</code> <code>None</code> Export JSON report.","path":["API Reference","audit()"],"tags":[]},{"location":"api/audit/#featureauditdetail","level":2,"title":"FeatureAuditDetail","text":"Attribute Type Description <code>.name</code> <code>str</code> The feature name. <code>.clean</code> <code>bool</code> No leakage detected. <code>.leaky_row_count</code> <code>int</code> Number of leaky rows. <code>.leaky_row_pct</code> <code>float</code> Fraction of rows with leakage (0.0–1.0). <code>.severity</code> <code>str</code> <code>\"HIGH\"</code>, <code>\"MEDIUM\"</code>, <code>\"LOW\"</code>, or <code>\"OK\"</code>. <code>.max_leakage</code> <code>timedelta \\| None</code> Largest time violation. <code>.median_leakage</code> <code>timedelta \\| None</code> Median time violation. <code>.total_rows</code> <code>int</code> Total rows examined. <code>.null_rows</code> <code>int</code> Rows where feature was NULL. <code>.leaky_rows</code> <code>DataFrame \\| None</code> DataFrame of up to 1,000 violating rows (when leakage detected). <p>Severity levels</p> <p>HIGH = &gt;5% leaky rows or max leakage &gt;7 days. MEDIUM = 1–5% or 1–7 days. LOW = &lt;1% and &lt;1 day. OK = no leakage.</p>","path":["API Reference","audit()"],"tags":[]},{"location":"api/build/","level":1,"title":"build()","text":"<p>Constructs a point-in-time correct training dataset.</p>","path":["API Reference","build()"],"tags":[]},{"location":"api/build/#timefence.build","level":2,"title":"<code>build(labels, features, output=None, *, max_lookback=DEFAULT_MAX_LOOKBACK, max_staleness=None, join='strict', on_missing=DEFAULT_ON_MISSING, splits=None, store=None, flatten_columns=False, progress=None)</code>","text":"<p>Build a point-in-time correct training set.</p> <p>Parameters:</p> Name Type Description Default <code>progress</code> <code>Callable[[str], None] | None</code> <p>Optional callback invoked with a status message at each step. Useful for progress bars. Called with messages like \"Loading labels\", \"Computing feature_name\", \"Joining feature_name\", \"Writing output\".</p> <code>None</code> Source code in <code>src/timefence/engine.py</code> <pre><code>def build(\n    labels: Labels,\n    features: Sequence[Feature | FeatureSet],\n    output: str | Path | None = None,\n    *,\n    max_lookback: str | timedelta = DEFAULT_MAX_LOOKBACK,\n    max_staleness: str | timedelta | None = None,\n    join: str = \"strict\",\n    on_missing: str = DEFAULT_ON_MISSING,\n    splits: dict[str, tuple[str, str]] | None = None,\n    store: Store | None = None,\n    flatten_columns: bool = False,\n    progress: Callable[[str], None] | None = None,\n) -&gt; BuildResult:\n    \"\"\"Build a point-in-time correct training set.\n\n    Args:\n        progress: Optional callback invoked with a status message at each step.\n            Useful for progress bars. Called with messages like \"Loading labels\",\n            \"Computing feature_name\", \"Joining feature_name\", \"Writing output\".\n    \"\"\"\n    start_time = time.time()\n\n    def _emit(msg: str) -&gt; None:\n        if progress is not None:\n            progress(msg)\n\n    max_lookback_td = parse_duration(max_lookback) or timedelta(\n        days=DEFAULT_MAX_LOOKBACK_DAYS\n    )\n    max_staleness_td = parse_duration(max_staleness)\n\n    if join not in (\"strict\", \"inclusive\"):\n        raise TimefenceConfigError(\n            f\"join must be 'strict' or 'inclusive', got '{join}'.\"\n        )\n    if on_missing not in (\"null\", \"skip\"):\n        raise TimefenceConfigError(\n            f\"on_missing must be 'null' or 'skip', got '{on_missing}'.\"\n        )\n\n    flat_features = flatten_features(features)\n\n    # Validate feature names are unique (both exact and after sanitization)\n    seen_names: dict[str, int] = {}\n    seen_safe: dict[str, list[str]] = {}  # safe_name -&gt; [original names]\n    for feat in flat_features:\n        seen_names[feat.name] = seen_names.get(feat.name, 0) + 1\n        safe = _safe_name(feat.name)\n        seen_safe.setdefault(safe, []).append(feat.name)\n    duplicates = {n: c for n, c in seen_names.items() if c &gt; 1}\n    if duplicates:\n        dup_str = \", \".join(f\"'{n}' (x{c})\" for n, c in duplicates.items())\n        raise TimefenceConfigError(\n            f\"Duplicate feature names: {dup_str}.\\n\\n\"\n            \"  Each feature must have a unique name. Duplicate names would cause\\n\"\n            \"  one feature to silently overwrite another.\\n\\n\"\n            \"  Fix: Set an explicit name on each feature:\\n\"\n            '    timefence.Feature(..., name=\"unique_name\")\\n'\n        )\n    collisions = {s: names for s, names in seen_safe.items() if len(set(names)) &gt; 1}\n    if collisions:\n        pairs = [f\"{sorted(set(names))}\" for names in collisions.values()]\n        raise TimefenceConfigError(\n            f\"Feature names collide after sanitization: {', '.join(pairs)}.\\n\\n\"\n            \"  These names are distinct but map to the same internal identifier,\\n\"\n            \"  which would cause one feature to silently overwrite another.\\n\\n\"\n            \"  Fix: Rename features to avoid ambiguity (e.g., use underscores consistently).\\n\"\n        )\n\n    for feat in flat_features:\n        if feat.embargo &gt;= max_lookback_td:\n            from timefence.errors import config_error_embargo_lookback\n\n            raise config_error_embargo_lookback(\n                format_duration(feat.embargo) or \"0d\",\n                format_duration(max_lookback_td) or DEFAULT_MAX_LOOKBACK,\n            )\n        if max_staleness_td is not None and max_staleness_td &lt;= feat.embargo:\n            raise TimefenceConfigError(\n                f\"max_staleness ({format_duration(max_staleness_td)}) must be greater than \"\n                f\"embargo ({format_duration(feat.embargo)}) for feature '{feat.name}'.\"\n            )\n\n    # Check build-level cache\n    if store is not None and output is not None:\n        label_hash = _content_hash_safe(labels.path, store)\n        feat_cache_keys = []\n        for feat in flat_features:\n            src_hash = _content_hash_safe(feat.source.path, store)\n            fck = store.feature_cache_key(\n                _definition_hash(feat), src_hash, format_duration(feat.embargo)\n            )\n            feat_cache_keys.append(fck)\n\n        bck = store.build_cache_key(\n            label_hash,\n            feat_cache_keys,\n            format_duration(max_lookback_td),\n            format_duration(max_staleness_td),\n            join,\n            on_missing,\n        )\n        cached_build = store.find_cached_build(bck)\n        if cached_build is not None:\n            elapsed = time.time() - start_time\n            cached_build[\"duration_seconds\"] = elapsed\n            return BuildResult(\n                output_path=cached_build.get(\"output\", {}).get(\"path\"),\n                manifest=cached_build,\n                stats=BuildStats(\n                    row_count=cached_build.get(\"output\", {}).get(\"row_count\", 0),\n                    column_count=cached_build.get(\"output\", {}).get(\"column_count\", 0),\n                    feature_stats={\n                        k: {\n                            \"matched\": v.get(\"matched_rows\", 0),\n                            \"missing\": v.get(\"missing_rows\", 0),\n                            \"cached\": True,\n                        }\n                        for k, v in cached_build.get(\"features\", {}).items()\n                    },\n                    duration_seconds=elapsed,\n                ),\n                sql=\"-- cached build\",\n            )\n\n    conn = duckdb.connect()\n    all_sql = []\n\n    try:\n        # Step 1: Register labels\n        _emit(\"Loading labels\")\n        if labels.path is not None:\n            _load_data_as_table(conn, labels.path, \"__labels_raw\")\n        elif labels.df is not None:\n            _load_data_as_table(conn, labels.df, \"__labels_raw\")\n        else:\n            raise TimefenceValidationError(\"Labels must have either path or df.\")\n\n        # Validate label schema\n        label_cols = [\n            col[0] for col in conn.execute(\"DESCRIBE __labels_raw\").fetchall()\n        ]\n        for key in labels.keys:\n            if key not in label_cols:\n                raise TimefenceSchemaError(\n                    f\"Labels missing key column '{key}'.\\n  Available: {label_cols}\"\n                )\n        if labels.label_time not in label_cols:\n            raise TimefenceSchemaError(\n                f\"Labels missing label_time column '{labels.label_time}'.\\n  Available: {label_cols}\"\n            )\n\n        # Add rowid for join tracking\n        conn.execute(\n            \"CREATE TEMP TABLE __labels AS \"\n            \"SELECT ROW_NUMBER() OVER () AS __label_rowid, * FROM __labels_raw\"\n        )\n        label_count = conn.execute(\"SELECT COUNT(*) FROM __labels\").fetchone()[0]\n        logger.info(\n            \"Labels: %d rows, keys=%s, label_time=%s\",\n            label_count,\n            labels.keys,\n            labels.label_time,\n        )\n\n        # Get label time range for manifest\n        time_range_row = conn.execute(\n            f\"SELECT MIN({_qi(labels.label_time)}), MAX({_qi(labels.label_time)}) FROM __labels\"\n        ).fetchone()\n        label_time_range = (\n            [str(time_range_row[0]), str(time_range_row[1])]\n            if time_range_row and time_range_row[0] is not None\n            else None\n        )\n\n        # Validate splits if provided\n        if splits:\n            _validate_splits(splits, conn, labels.label_time)\n\n        # Step 2: Register sources and compute features\n        registered_sources: dict[str, str] = {}\n        feature_tables: dict[str, tuple[str, list[str]]] = {}\n        feature_cache_keys: list[str] = []\n        feature_cache_status: dict[str, bool] = {}  # name -&gt; was_cached\n\n        for i, feat in enumerate(flat_features, 1):\n            _emit(f\"Computing {feat.name} ({i}/{len(flat_features)})\")\n            src_name = feat.source.name\n            if src_name not in registered_sources:\n                table_name = f\"__src_{_safe_name(src_name)}\"\n                _register_source(conn, feat.source, table_name)\n                registered_sources[src_name] = table_name\n\n            src_table = registered_sources[src_name]\n            _validate_source_schema(conn, src_table, feat, labels.keys)\n            _check_duplicates(conn, src_table, feat)\n\n            feat_table = f\"__feat_{_safe_name(feat.name)}\"\n\n            # Check feature-level cache\n            cached = False\n            fck = None\n            if store is not None:\n                src_hash = _content_hash_safe(feat.source.path, store)\n                fck = store.feature_cache_key(\n                    _definition_hash(feat), src_hash, format_duration(feat.embargo)\n                )\n                feature_cache_keys.append(fck)\n                if store.has_feature_cache(feat.name, fck):\n                    cache_path = store.feature_cache_path(feat.name, fck)\n                    conn.execute(\n                        f\"CREATE OR REPLACE TEMP TABLE {feat_table} AS \"\n                        f\"SELECT * FROM read_parquet({_ql(cache_path)})\"\n                    )\n                    feat_cols = [\n                        c[0] for c in conn.execute(f\"DESCRIBE {feat_table}\").fetchall()\n                    ]\n                    output_cols = [\n                        c\n                        for c in feat_cols\n                        if c != \"feature_time\" and c not in feat.source_keys\n                    ]\n                    cached = True\n                    feature_cache_status[feat.name] = True\n\n            if not cached:\n                feature_cache_status[feat.name] = False\n                feat_sqls, output_cols = _compute_feature_table(\n                    conn, feat, src_table, feat_table\n                )\n                all_sql.extend(feat_sqls)\n                for s in feat_sqls:\n                    logger.info(\"Feature SQL [%s]:\\n  %s\", feat.name, s)\n\n                # Save freshly computed feature to cache\n                if store is not None and fck is not None:\n                    cache_path = store.feature_cache_path(feat.name, fck)\n                    try:\n                        conn.execute(\n                            f\"COPY (SELECT * FROM {feat_table}) TO {_ql(cache_path)} (FORMAT PARQUET)\"\n                        )\n                    except (duckdb.Error, OSError) as exc:\n                        logger.warning(\n                            \"Feature cache write failed for %s: %s\", feat.name, exc\n                        )\n            else:\n                logger.info(\"Feature [%s]: loaded from cache\", feat.name)\n\n            feature_tables[feat.name] = (feat_table, output_cols)\n\n            # Timezone validation\n            if output_cols:\n                _validate_timezones(conn, labels.label_time, feat, feat_table)\n\n        # Step 3: Point-in-time joins\n        for i, feat in enumerate(flat_features, 1):\n            _emit(f\"Joining {feat.name} ({i}/{len(flat_features)})\")\n            feat_table, output_cols = feature_tables[feat.name]\n            join_sql, strategy = _build_join_sql(\n                feat,\n                feat_table,\n                labels.keys,\n                labels.label_time,\n                join,\n                max_lookback_td,\n                max_staleness_td,\n                output_cols,\n            )\n            logger.info(\n                \"Join SQL [%s] (strategy=%s):\\n  %s\", feat.name, strategy, join_sql\n            )\n            try:\n                conn.execute(join_sql)\n            except duckdb.Error as exc:\n                # ASOF fallback: if ASOF fails, retry with ROW_NUMBER\n                if strategy == \"asof\":\n                    logger.debug(\n                        \"ASOF JOIN failed for %s, falling back to ROW_NUMBER: %s\",\n                        feat.name,\n                        exc,\n                    )\n                    join_sql = _build_row_number_join_sql(\n                        feat,\n                        feat_table,\n                        labels.keys,\n                        labels.label_time,\n                        join,\n                        max_lookback_td,\n                        max_staleness_td,\n                        output_cols,\n                    )\n                    conn.execute(join_sql)\n                    strategy = \"row_number\"\n                else:\n                    raise\n            all_sql.append(join_sql)\n\n        # Step 4: Combine all joins\n        key_cols = \", \".join(f\"l.{_qi(k)}\" for k in labels.keys)\n        target_cols = \", \".join(f\"l.{_qi(t)}\" for t in labels.target)\n        join_clauses = []\n        select_cols = [key_cols, f\"l.{_qi(labels.label_time)}\", target_cols]\n\n        for feat in flat_features:\n            prefix = feat.name\n            safe_prefix = _safe_name(prefix)\n            _, output_cols = feature_tables[feat.name]\n            for col in output_cols:\n                select_cols.append(f\"j_{safe_prefix}.{_qi(f'{prefix}__{col}')}\")\n            join_clauses.append(\n                f\"LEFT JOIN __joined_{safe_prefix} j_{safe_prefix} \"\n                f\"ON l.__label_rowid = j_{safe_prefix}.__label_rowid\"\n            )\n\n        order_cols = (\n            \", \".join(f\"l.{_qi(k)}\" for k in labels.keys)\n            + f\", l.{_qi(labels.label_time)}\"\n        )\n        final_sql = (\n            f\"SELECT {', '.join(select_cols)} \"\n            f\"FROM __labels l \"\n            f\"{' '.join(join_clauses)} \"\n            f\"ORDER BY {order_cols}\"\n        )\n\n        # Handle on_missing=\"skip\"\n        if on_missing == \"skip\":\n            not_null_conditions = []\n            for feat in flat_features:\n                safe_pref = _safe_name(feat.name)\n                _, output_cols = feature_tables[feat.name]\n                for col in output_cols:\n                    not_null_conditions.append(\n                        f\"j_{safe_pref}.{_qi(f'{feat.name}__{col}')} IS NOT NULL\"\n                    )\n            if not_null_conditions:\n                final_sql = (\n                    f\"SELECT {', '.join(select_cols)} \"\n                    f\"FROM __labels l \"\n                    f\"{' '.join(join_clauses)} \"\n                    f\"WHERE {' AND '.join(not_null_conditions)} \"\n                    f\"ORDER BY {order_cols}\"\n                )\n\n        all_sql.append(final_sql)\n        logger.info(\"Final SQL:\\n  %s\", final_sql)\n\n        # Flatten column names if requested\n        if flatten_columns:\n            result_rel = conn.execute(final_sql)\n            col_descriptions = result_rel.description\n            seen: set[str] = set()\n            can_flatten = True\n            for desc in col_descriptions:\n                name = desc[0]\n                short = name.split(\"__\", 1)[1] if \"__\" in name else name\n                if short in seen:\n                    can_flatten = False\n                    break\n                seen.add(short)\n\n            if can_flatten:\n                renames = []\n                for desc in col_descriptions:\n                    name = desc[0]\n                    if \"__\" in name:\n                        short = name.split(\"__\", 1)[1]\n                        renames.append(f\"{_qi(name)} AS {_qi(short)}\")\n                    else:\n                        renames.append(_qi(name))\n                final_sql = f\"SELECT {', '.join(renames)} FROM ({final_sql})\"\n\n        # Step 5: Materialize result, write output, collect stats\n        _emit(\"Writing output\")\n        conn.execute(f\"CREATE TEMP TABLE __result AS {final_sql}\")\n        result_cols = [c[0] for c in conn.execute(\"DESCRIBE __result\").fetchall()]\n        result_count = conn.execute(\"SELECT COUNT(*) FROM __result\").fetchone()[0]\n\n        if output is not None:\n            output = str(output)\n            Path(output).parent.mkdir(parents=True, exist_ok=True)\n            conn.execute(\n                f\"COPY (SELECT * FROM __result) TO {_ql(output)} (FORMAT PARQUET)\"\n            )\n\n        feature_stats = {}\n        for feat in flat_features:\n            prefix = feat.name\n            _, output_cols = feature_tables[feat.name]\n            if output_cols:\n                first_col = f\"{prefix}__{output_cols[0]}\"\n                if flatten_columns and output_cols[0] in result_cols:\n                    first_col = output_cols[0]\n                try:\n                    null_count = conn.execute(\n                        f\"SELECT COUNT(*) FROM __result WHERE {_qi(first_col)} IS NULL\"\n                    ).fetchone()[0]\n                except duckdb.Error as exc:\n                    logger.debug(\n                        \"Could not compute null count for %s: %s\", feat.name, exc\n                    )\n                    null_count = 0\n                feature_stats[feat.name] = {\n                    \"matched\": result_count - null_count,\n                    \"missing\": null_count,\n                    \"cached\": feature_cache_status.get(feat.name, False),\n                }\n\n        # Step 6: Post-build verification\n        _emit(\"Verifying temporal correctness\")\n        audit_passed = True\n        for feat in flat_features:\n            prefix = feat.name\n            safe_prefix = _safe_name(prefix)\n            ft_col = _qi(f\"{prefix}__feature_time\")\n            lt = _qi(labels.label_time)\n            embargo_interval = duration_to_sql_interval(feat.embargo)\n\n            if join == \"strict\":\n                if feat.embargo.total_seconds() &gt; 0:\n                    check_sql = (\n                        f\"SELECT COUNT(*) FROM __joined_{safe_prefix} j \"\n                        f\"JOIN __labels l ON j.__label_rowid = l.__label_rowid \"\n                        f\"WHERE j.{ft_col} IS NOT NULL \"\n                        f\"AND j.{ft_col} &gt;= l.{lt} - {embargo_interval}\"\n                    )\n                else:\n                    check_sql = (\n                        f\"SELECT COUNT(*) FROM __joined_{safe_prefix} j \"\n                        f\"JOIN __labels l ON j.__label_rowid = l.__label_rowid \"\n                        f\"WHERE j.{ft_col} IS NOT NULL \"\n                        f\"AND j.{ft_col} &gt;= l.{lt}\"\n                    )\n            else:\n                if feat.embargo.total_seconds() &gt; 0:\n                    check_sql = (\n                        f\"SELECT COUNT(*) FROM __joined_{safe_prefix} j \"\n                        f\"JOIN __labels l ON j.__label_rowid = l.__label_rowid \"\n                        f\"WHERE j.{ft_col} IS NOT NULL \"\n                        f\"AND j.{ft_col} &gt; l.{lt} - {embargo_interval}\"\n                    )\n                else:\n                    check_sql = (\n                        f\"SELECT COUNT(*) FROM __joined_{safe_prefix} j \"\n                        f\"JOIN __labels l ON j.__label_rowid = l.__label_rowid \"\n                        f\"WHERE j.{ft_col} IS NOT NULL \"\n                        f\"AND j.{ft_col} &gt; l.{lt}\"\n                    )\n            violations = conn.execute(check_sql).fetchone()[0]\n            if violations &gt; 0:\n                audit_passed = False\n\n        # Handle splits\n        split_paths = None\n        if splits and output:\n            split_paths = {}\n            output_path = Path(output)\n            for split_name, (start, end) in splits.items():\n                split_file = (\n                    output_path.parent\n                    / f\"{output_path.stem}_{split_name}{output_path.suffix}\"\n                )\n                split_sql = (\n                    f\"COPY (SELECT * FROM ({final_sql}) \"\n                    f\"WHERE {_qi(labels.label_time)} &gt;= {_ql(start)} \"\n                    f\"AND {_qi(labels.label_time)} &lt; {_ql(end)}) \"\n                    f\"TO {_ql(split_file)} (FORMAT PARQUET)\"\n                )\n                conn.execute(split_sql)\n                split_paths[split_name] = split_file\n\n        elapsed = time.time() - start_time\n\n        stats = BuildStats(\n            row_count=result_count,\n            column_count=len(result_cols),\n            feature_stats=feature_stats,\n            duration_seconds=elapsed,\n        )\n\n        # Compute output file size\n        output_file_size = None\n        if output is not None:\n            import contextlib\n\n            with contextlib.suppress(OSError):\n                output_file_size = Path(output).stat().st_size\n\n        # Build manifest\n        build_id = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n        manifest = {\n            \"timefence_version\": __version__,\n            \"build_id\": build_id,\n            \"created_at\": datetime.now(timezone.utc).isoformat(),\n            \"duration_seconds\": elapsed,\n            \"labels\": {\n                \"path\": str(labels.path) if labels.path else None,\n                \"content_hash\": _content_hash_safe(labels.path, store),\n                \"row_count\": label_count,\n                \"time_range\": label_time_range,\n                \"keys\": labels.keys,\n                \"label_time_column\": labels.label_time,\n                \"target_columns\": labels.target,\n            },\n            \"features\": {},\n            \"parameters\": {\n                \"max_lookback\": format_duration(max_lookback_td),\n                \"max_staleness\": format_duration(max_staleness_td),\n                \"join\": join,\n                \"on_missing\": on_missing,\n            },\n            \"output\": {\n                \"path\": str(output) if output else None,\n                \"content_hash\": _content_hash_safe(\n                    Path(output) if output else None, store\n                ),\n                \"row_count\": result_count,\n                \"column_count\": len(result_cols),\n                \"file_size_bytes\": output_file_size,\n            },\n            \"audit\": {\n                \"passed\": audit_passed,\n                \"invariant\": f\"feature_time {'&lt;' if join == 'strict' else '&lt;='} label_time - embargo\",\n                \"rows_checked\": result_count,\n            },\n            \"environment\": {\n                \"python_version\": _python_version(),\n                \"duckdb_version\": duckdb.__version__,\n                \"os\": _os_identifier(),\n            },\n        }\n        for feat in flat_features:\n            fstats = feature_stats.get(feat.name, {})\n            manifest[\"features\"][feat.name] = {\n                \"definition_hash\": _definition_hash(feat),\n                \"source_content_hash\": _content_hash_safe(feat.source.path, store),\n                \"embargo\": format_duration(feat.embargo),\n                \"matched_rows\": fstats.get(\"matched\", 0),\n                \"missing_rows\": fstats.get(\"missing\", 0),\n                \"output_columns\": feature_tables[feat.name][1],\n                \"cached\": feature_cache_status.get(feat.name, False),\n            }\n\n        # Store build cache key in manifest for future lookups\n        if store is not None and feature_cache_keys:\n            bck = store.build_cache_key(\n                _content_hash_safe(labels.path, store),\n                feature_cache_keys,\n                format_duration(max_lookback_td),\n                format_duration(max_staleness_td),\n                join,\n                on_missing,\n            )\n            manifest[\"build_cache_key\"] = bck\n            manifest_path = store.save_build(manifest)\n            manifest[\"manifest_path\"] = str(manifest_path)\n\n        return BuildResult(\n            output_path=str(output) if output else None,\n            manifest=manifest,\n            stats=stats,\n            splits=split_paths,\n            sql=\"\\n\\n\".join(all_sql),\n        )\n\n    finally:\n        conn.close()\n</code></pre>","path":["API Reference","build()"],"tags":[]},{"location":"api/build/#parameters","level":2,"title":"Parameters","text":"Parameter Type Default Description <code>labels</code> <code>Labels</code> required Label definition. <code>features</code> <code>Sequence[Feature \\| FeatureSet]</code> required Features to join. <code>output</code> <code>str \\| Path \\| None</code> <code>None</code> Output file path. If <code>None</code>, no file is written. <code>max_lookback</code> <code>str \\| timedelta</code> <code>\"365d\"</code> Maximum feature age. <code>max_staleness</code> <code>str \\| timedelta \\| None</code> <code>None</code> Max feature age before treating as missing. <code>join</code> <code>str</code> <code>\"strict\"</code> <code>\"strict\"</code> (<code>&lt;</code>) or <code>\"inclusive\"</code> (<code>&lt;=</code>). <code>on_missing</code> <code>str</code> <code>\"null\"</code> <code>\"null\"</code> (keep row with NULLs) or <code>\"skip\"</code> (drop row). <code>splits</code> <code>dict \\| None</code> <code>None</code> Time-based splits: <code>{\"train\": (\"start\", \"end\"), ...}</code>. <code>store</code> <code>Store \\| None</code> <code>None</code> Build tracking and caching. <code>flatten_columns</code> <code>bool</code> <code>False</code> Strip feature name prefix from output columns. <code>progress</code> <code>Callable \\| None</code> <code>None</code> Callback for progress reporting.","path":["API Reference","build()"],"tags":[]},{"location":"api/build/#returns-buildresult","level":2,"title":"Returns: BuildResult","text":"Attribute Type Description <code>.output_path</code> <code>str \\| None</code> Path to the output file. <code>.stats</code> <code>BuildStats</code> Build statistics (see below). <code>.sql</code> <code>str</code> The exact SQL queries executed. <code>.splits</code> <code>dict[str, Path] \\| None</code> Split output file paths. <code>.manifest</code> <code>dict</code> Full build manifest (JSON-serializable). <code>.validate()</code> <code>bool</code> Check if the post-build audit passed. <code>.explain()</code> <code>str</code> Return the SQL used for joins.","path":["API Reference","build()"],"tags":[]},{"location":"api/build/#buildstats","level":2,"title":"BuildStats","text":"Attribute Type Description <code>.row_count</code> <code>int</code> Total rows in the output dataset. <code>.column_count</code> <code>int</code> Total columns in the output dataset. <code>.feature_stats</code> <code>dict[str, dict]</code> Per-feature join statistics (see below). <code>.duration_seconds</code> <code>float</code> Wall-clock time for the build. <p>Each entry in <code>.feature_stats</code> is a dict:</p> Key Type Description <code>\"matched\"</code> <code>int</code> Number of label rows that matched a feature value. <code>\"missing\"</code> <code>int</code> Number of label rows with no valid feature (filled with NULL or skipped). <code>\"cached\"</code> <code>bool</code> Whether this feature was loaded from cache.","path":["API Reference","build()"],"tags":[]},{"location":"api/build/#build-manifest","level":2,"title":"Build manifest","text":"<p>Every build produces a JSON manifest (saved by <code>Store</code> or accessible via <code>.manifest</code>):</p> <pre><code>{\n  \"timefence_version\": \"0.9.1\",\n  \"build_id\": \"20240315T120000Z\",\n  \"created_at\": \"2024-03-15T12:00:00+00:00\",\n  \"duration_seconds\": 1.8,\n  \"labels\": {\n    \"path\": \"data/labels.parquet\",\n    \"content_hash\": \"sha256:abc123...\",\n    \"row_count\": 5000,\n    \"time_range\": [\"2023-01-01\", \"2024-12-31\"],\n    \"keys\": [\"user_id\"],\n    \"label_time_column\": \"label_time\",\n    \"target_columns\": [\"churned\"]\n  },\n  \"features\": {\n    \"rolling_spend_30d\": {\n      \"definition_hash\": \"sha256:def456...\",\n      \"source_content_hash\": \"sha256:aaa789...\",\n      \"embargo\": \"1d\",\n      \"matched_rows\": 4800,\n      \"missing_rows\": 200,\n      \"output_columns\": [\"spend_30d\"],\n      \"cached\": false\n    }\n  },\n  \"parameters\": {\n    \"max_lookback\": \"365d\",\n    \"max_staleness\": null,\n    \"join\": \"strict\",\n    \"on_missing\": \"null\"\n  },\n  \"output\": {\n    \"path\": \"train.parquet\",\n    \"content_hash\": \"sha256:out321...\",\n    \"file_size_bytes\": 204800,\n    \"row_count\": 5000,\n    \"column_count\": 7\n  },\n  \"audit\": {\n    \"passed\": true,\n    \"invariant\": \"feature_time &lt; label_time - embargo\",\n    \"rows_checked\": 5000\n  },\n  \"environment\": {\n    \"python_version\": \"3.11.5\",\n    \"duckdb_version\": \"0.10.1\",\n    \"os\": \"Linux-6.1.0-x86_64\"\n  }\n}\n</code></pre>","path":["API Reference","build()"],"tags":[]},{"location":"api/build/#column-naming","level":2,"title":"Column naming","text":"<p>By default, feature columns in the output are namespaced as <code>{feature_name}__{column_name}</code> to avoid collisions:</p> <pre><code>user_id | label_time | churned | country__country | spend__spend_30d\n</code></pre> <p>Set <code>flatten_columns=True</code> to strip the prefix when there are no name collisions:</p> <pre><code>user_id | label_time | churned | country | spend_30d\n</code></pre>","path":["API Reference","build()"],"tags":[]},{"location":"api/build/#progress-reporting","level":2,"title":"Progress reporting","text":"<p>Pass a callback to show build progress (used by the CLI for its Rich progress bar):</p> <pre><code>def on_progress(message: str):\n    print(message)\n\nresult = timefence.build(\n    labels=labels,\n    features=features,\n    output=\"train.parquet\",\n    progress=on_progress,\n)\n# Prints: \"Loading labels\", \"Computing spend (1/2)\", \"Joining spend (1/2)\", ...\n</code></pre>","path":["API Reference","build()"],"tags":[]},{"location":"api/diff/","level":1,"title":"diff()","text":"<p>Compare two training datasets for schema drift and value changes.</p>","path":["API Reference","diff()"],"tags":[]},{"location":"api/diff/#timefence.diff","level":2,"title":"<code>diff(old, new, *, keys, label_time, atol=DEFAULT_ATOL, rtol=DEFAULT_RTOL)</code>","text":"<p>Compare two training datasets.</p> Source code in <code>src/timefence/engine.py</code> <pre><code>def diff(\n    old: str | Path,\n    new: str | Path,\n    *,\n    keys: str | list[str],\n    label_time: str,\n    atol: float = DEFAULT_ATOL,\n    rtol: float = DEFAULT_RTOL,\n) -&gt; DiffResult:\n    \"\"\"Compare two training datasets.\"\"\"\n    keys_list = [keys] if isinstance(keys, str) else list(keys)\n\n    conn = duckdb.connect()\n    try:\n        conn.execute(\n            f\"CREATE TEMP TABLE __old AS SELECT * FROM read_parquet({_ql(old)})\"\n        )\n        conn.execute(\n            f\"CREATE TEMP TABLE __new AS SELECT * FROM read_parquet({_ql(new)})\"\n        )\n\n        old_count = conn.execute(\"SELECT COUNT(*) FROM __old\").fetchone()[0]\n        new_count = conn.execute(\"SELECT COUNT(*) FROM __new\").fetchone()[0]\n\n        old_cols = {c[0] for c in conn.execute(\"DESCRIBE __old\").fetchall()}\n        new_cols = {c[0] for c in conn.execute(\"DESCRIBE __new\").fetchall()}\n\n        result = DiffResult(old_rows=old_count, new_rows=new_count)\n\n        meta_cols = set(keys_list) | {label_time}\n        added = new_cols - old_cols\n        removed = old_cols - new_cols\n        common = (old_cols &amp; new_cols) - meta_cols\n\n        for col in sorted(added):\n            result.schema_changes.append(\n                {\"type\": \"+\", \"column\": col, \"detail\": \"(new column)\"}\n            )\n        for col in sorted(removed):\n            result.schema_changes.append(\n                {\"type\": \"-\", \"column\": col, \"detail\": \"(removed)\"}\n            )\n\n        key_join = \" AND \".join(f\"o.{_qi(k)} = n.{_qi(k)}\" for k in keys_list)\n        key_join += f\" AND o.{_qi(label_time)} = n.{_qi(label_time)}\"\n\n        for col in sorted(common):\n            qc = _qi(col)\n            try:\n                # Try tolerance-aware numeric comparison first\n                try:\n                    change_sql = (\n                        f\"SELECT COUNT(*) FROM __old o JOIN __new n ON {key_join} \"\n                        f\"WHERE o.{qc} IS NOT NULL AND n.{qc} IS NOT NULL \"\n                        f\"AND ABS(CAST(o.{qc} AS DOUBLE) - CAST(n.{qc} AS DOUBLE)) \"\n                        f\"&gt; {atol} + {rtol} * ABS(CAST(n.{qc} AS DOUBLE))\"\n                    )\n                    changed = conn.execute(change_sql).fetchone()[0]\n                    # Also count null-vs-non-null differences\n                    null_diff_sql = (\n                        f\"SELECT COUNT(*) FROM __old o JOIN __new n ON {key_join} \"\n                        f\"WHERE (o.{qc} IS NULL) != (n.{qc} IS NULL)\"\n                    )\n                    changed += conn.execute(null_diff_sql).fetchone()[0]\n                except (duckdb.Error, duckdb.ConversionException):\n                    # Non-numeric: fall back to exact comparison\n                    change_sql = (\n                        f\"SELECT COUNT(*) FROM __old o JOIN __new n ON {key_join} \"\n                        f\"WHERE o.{qc} IS DISTINCT FROM n.{qc}\"\n                    )\n                    changed = conn.execute(change_sql).fetchone()[0]\n\n                if changed &gt; 0:\n                    joined = min(old_count, new_count)\n                    pct = changed / joined if joined &gt; 0 else 0\n\n                    stats_entry: dict[str, Any] = {\n                        \"changed_count\": changed,\n                        \"changed_pct\": pct,\n                    }\n\n                    # Compute numeric delta stats when possible\n                    try:\n                        delta_sql = (\n                            f\"SELECT \"\n                            f\"AVG(CAST(n.{qc} AS DOUBLE) - CAST(o.{qc} AS DOUBLE)), \"\n                            f\"MAX(ABS(CAST(n.{qc} AS DOUBLE) - CAST(o.{qc} AS DOUBLE))) \"\n                            f\"FROM __old o JOIN __new n ON {key_join} \"\n                            f\"WHERE o.{qc} IS DISTINCT FROM n.{qc}\"\n                        )\n                        delta_row = conn.execute(delta_sql).fetchone()\n                        if delta_row and delta_row[0] is not None:\n                            stats_entry[\"mean_delta\"] = float(delta_row[0])\n                            stats_entry[\"max_delta\"] = float(delta_row[1])\n                    except (duckdb.Error, TypeError):\n                        pass  # Non-numeric column, delta stats not applicable\n\n                    result.value_changes[col] = stats_entry\n                    result.schema_changes.append(\n                        {\n                            \"type\": \"~\",\n                            \"column\": col,\n                            \"detail\": f\"{changed} values changed ({pct:.1%})\",\n                        }\n                    )\n                else:\n                    result.schema_changes.append(\n                        {\"type\": \"=\", \"column\": col, \"detail\": \"unchanged\"}\n                    )\n            except (duckdb.Error, TypeError) as exc:\n                logger.warning(\"Column comparison failed for %s: %s\", col, exc)\n                result.schema_changes.append(\n                    {\"type\": \"?\", \"column\": col, \"detail\": \"comparison failed\"}\n                )\n\n        return result\n    finally:\n        conn.close()\n</code></pre>","path":["API Reference","diff()"],"tags":[]},{"location":"api/diff/#example","level":2,"title":"Example","text":"<pre><code>result = timefence.diff(\n    old=\"train_v1.parquet\",\n    new=\"train_v2.parquet\",\n    keys=[\"user_id\"],\n    label_time=\"label_time\",\n    atol=1e-10,\n    rtol=1e-7,\n)\n</code></pre>","path":["API Reference","diff()"],"tags":[]},{"location":"api/diff/#returns-diffresult","level":2,"title":"Returns: DiffResult","text":"Attribute Type Description <code>.old_rows</code> <code>int</code> Row count in old dataset. <code>.new_rows</code> <code>int</code> Row count in new dataset. <code>.schema_changes</code> <code>list[dict]</code> Schema changes: <code>type</code> (<code>+</code> added, <code>-</code> removed, <code>~</code> changed, <code>=</code> unchanged, <code>?</code> comparison failed), <code>column</code>, <code>detail</code>. <code>.value_changes</code> <code>dict[str, dict]</code> Per-column: <code>changed_count</code>, <code>changed_pct</code>, <code>mean_delta</code>, <code>max_delta</code>. <code>mean_delta</code> and <code>max_delta</code> are only present for numeric columns.","path":["API Reference","diff()"],"tags":[]},{"location":"api/explain/","level":1,"title":"explain()","text":"<p>Preview the join logic that <code>build()</code> will use, without executing any queries.</p>","path":["API Reference","explain()"],"tags":[]},{"location":"api/explain/#timefence.explain","level":2,"title":"<code>explain(labels, features, *, max_lookback=DEFAULT_MAX_LOOKBACK, max_staleness=None, join='strict')</code>","text":"<p>Preview join logic without executing.</p> Source code in <code>src/timefence/engine.py</code> <pre><code>def explain(\n    labels: Labels,\n    features: Sequence[Feature | FeatureSet],\n    *,\n    max_lookback: str | timedelta = DEFAULT_MAX_LOOKBACK,\n    max_staleness: str | timedelta | None = None,\n    join: str = \"strict\",\n) -&gt; ExplainResult:\n    \"\"\"Preview join logic without executing.\"\"\"\n    max_lookback_td = parse_duration(max_lookback) or timedelta(\n        days=DEFAULT_MAX_LOOKBACK_DAYS\n    )\n    flat_features = flatten_features(features)\n\n    conn = duckdb.connect()\n    try:\n        if labels.path is not None:\n            label_count = conn.execute(\n                f\"SELECT COUNT(*) FROM read_parquet({_ql(labels.path)})\"\n            ).fetchone()[0]\n        elif labels.df is not None:\n            conn.register(\"__lbl\", labels.df)\n            label_count = conn.execute(\"SELECT COUNT(*) FROM __lbl\").fetchone()[0]\n        else:\n            label_count = 0\n    finally:\n        conn.close()\n\n    result = ExplainResult(label_count=label_count)\n\n    for feat in flat_features:\n        embargo_str = format_duration(feat.embargo) or \"none\"\n        lookback_str = format_duration(max_lookback_td)\n        strategy = \"asof\" if _use_asof_strategy(feat) else \"row_number\"\n\n        op = \"&lt;\" if join == \"strict\" else \"&lt;=\"\n\n        if feat.embargo.total_seconds() &gt; 0:\n            join_cond = f\"feature_time {op} label_time - INTERVAL '{embargo_str}'\"\n            window = f\"[label_time - {lookback_str}, label_time - {embargo_str})\"\n        else:\n            join_cond = f\"feature_time {op} label_time\"\n            window = f\"[label_time - {lookback_str}, label_time)\"\n\n        source_ref = str(feat.source.path) if feat.source.path else feat.source.name\n        key_placeholder = \"{K}\"\n        time_placeholder = \"{T}\"\n\n        if feat.mode == \"columns\":\n            cols = \", \".join(feat._columns.values())\n            ts = feat.source.timestamp\n            key_col = feat.source_keys[0]\n            embargo_clause = (\n                f\" - INTERVAL '{embargo_str}'\"\n                if feat.embargo.total_seconds() &gt; 0\n                else \"\"\n            )\n            example_sql = (\n                f\"SELECT {key_col}, {ts} AS feature_time, {cols}\\n\"\n                f\"FROM '{source_ref}'\\n\"\n                f\"WHERE {key_col} = {key_placeholder}\\n\"\n                f\"  AND {ts} {op} {time_placeholder}{embargo_clause}\\n\"\n                f\"  AND {ts} &gt;= {time_placeholder} - INTERVAL '{lookback_str}'\\n\"\n                f\"ORDER BY {ts} DESC\\nLIMIT 1\"\n            )\n        elif feat.mode == \"sql\":\n            example_sql = f\"WITH feature AS (\\n  {feat._sql_text.strip()}\\n)\\nSELECT * FROM feature\\n...\"\n        else:\n            example_sql = f\"-- Python transform: {feat._transform.__name__}\"\n\n        result.plan.append(\n            {\n                \"name\": feat.name,\n                \"source\": source_ref,\n                \"join_condition\": join_cond,\n                \"window\": window,\n                \"embargo_str\": (\n                    embargo_str if feat.embargo.total_seconds() &gt; 0 else \"none\"\n                ),\n                \"strategy\": strategy,\n                \"sql\": example_sql,\n            }\n        )\n\n    return result\n</code></pre>","path":["API Reference","explain()"],"tags":[]},{"location":"api/explain/#example","level":2,"title":"Example","text":"<pre><code>import timefence\n\nresult = timefence.explain(\n    labels=labels,\n    features=[rolling_spend, user_country],\n    max_lookback=\"365d\",\n    join=\"strict\",\n)\n\nprint(result)\n</code></pre>","path":["API Reference","explain()"],"tags":[]},{"location":"api/explain/#sample-output","level":3,"title":"Sample output","text":"<pre><code>JOIN PLAN for 5000 label rows\n\nFor each label row (keys, label_time):\n\n  1. rolling_spend_30d\n     Source:  data/transactions.parquet\n     Join:    feature_time &lt; label_time - INTERVAL '1d'\n     Window:  [label_time - 365d, label_time - 1d)\n     Embargo: 1d\n     Strategy: row_number\n     SQL:\n       SELECT user_id, created_at AS feature_time, amount\n       FROM 'data/transactions.parquet'\n       WHERE user_id = {K}\n         AND created_at &lt; {T} - INTERVAL '1d'\n         AND created_at &gt;= {T} - INTERVAL '365d'\n       ORDER BY created_at DESC\n       LIMIT 1\n\n  2. country\n     Source:  data/users.parquet\n     Join:    feature_time &lt; label_time\n     Window:  [label_time - 365d, label_time)\n     Embargo: none\n     Strategy: asof\n     SQL:\n       SELECT user_id, updated_at AS feature_time, country\n       FROM 'data/users.parquet'\n       WHERE user_id = {K}\n         AND updated_at &lt; {T}\n         AND updated_at &gt;= {T} - INTERVAL '365d'\n       ORDER BY updated_at DESC\n       LIMIT 1\n</code></pre> <p><code>{K}</code> and <code>{T}</code> are placeholders for the entity key and label time of each row.</p>","path":["API Reference","explain()"],"tags":[]},{"location":"api/explain/#parameters","level":2,"title":"Parameters","text":"Parameter Type Default Description <code>labels</code> <code>Labels</code> required Label definition. <code>features</code> <code>Sequence[Feature \\| FeatureSet]</code> required Features to explain. <code>max_lookback</code> <code>str \\| timedelta</code> <code>\"365d\"</code> Maximum feature age. <code>max_staleness</code> <code>str \\| timedelta \\| None</code> <code>None</code> Max staleness threshold. <code>join</code> <code>str</code> <code>\"strict\"</code> <code>\"strict\"</code> (<code>&lt;</code>) or <code>\"inclusive\"</code> (<code>&lt;=</code>).","path":["API Reference","explain()"],"tags":[]},{"location":"api/explain/#returns-explainresult","level":2,"title":"Returns: ExplainResult","text":"Attribute Type Description <code>.label_count</code> <code>int</code> Number of label rows. <code>.plan</code> <code>list[dict]</code> Per-feature join plan (see below). <p>Each item in <code>.plan</code> contains:</p> Key Type Description <code>name</code> <code>str</code> Feature name. <code>source</code> <code>str</code> Source file path or name. <code>join_condition</code> <code>str</code> The temporal join condition (e.g., <code>feature_time &lt; label_time</code>). <code>window</code> <code>str</code> The valid feature window (e.g., <code>[label_time - 365d, label_time)</code>). <code>embargo_str</code> <code>str</code> Embargo duration or <code>\"none\"</code>. <code>strategy</code> <code>str</code> <code>\"asof\"</code> or <code>\"row_number\"</code>. <code>sql</code> <code>str</code> Example SQL for this feature.","path":["API Reference","explain()"],"tags":[]},{"location":"api/feature/","level":1,"title":"Feature","text":"<p>A named signal derived from a Source. Exactly one of <code>columns</code>, <code>sql</code>, or <code>transform</code> must be provided.</p>","path":["API Reference","Feature"],"tags":[]},{"location":"api/feature/#timefence.Feature","level":2,"title":"<code>Feature(source, *, columns=None, sql=None, transform=None, name=None, embargo=None, key_mapping=None, on_duplicate='error')</code>","text":"<p>A named, versioned column derived from a source.</p> <p>Exactly one of columns, sql, or transform must be provided.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>SourceType</code> <p>The data source for this feature.</p> required <code>columns</code> <code>str | list[str] | dict[str, str] | None</code> <p>Column name(s) to select (Mode 1).</p> <code>None</code> <code>sql</code> <code>str | Path | None</code> <p>SQL query string or path to .sql file (Mode 2).</p> <code>None</code> <code>transform</code> <code>Callable | None</code> <p>Python callable (Mode 3).</p> <code>None</code> <code>name</code> <code>str | None</code> <p>Feature name (auto-derived if possible).</p> <code>None</code> <code>embargo</code> <code>str | timedelta | None</code> <p>Computation lag buffer (e.g., \"1d\").</p> <code>None</code> <code>key_mapping</code> <code>dict[str, str] | None</code> <p>Map label key names to source key names.</p> <code>None</code> <code>on_duplicate</code> <code>str</code> <p>\"error\" (default) or \"keep_any\".</p> <code>'error'</code> Source code in <code>src/timefence/core.py</code> <pre><code>def __init__(\n    self,\n    source: SourceType,\n    *,\n    columns: str | list[str] | dict[str, str] | None = None,\n    sql: str | Path | None = None,\n    transform: Callable | None = None,\n    name: str | None = None,\n    embargo: str | timedelta | None = None,\n    key_mapping: dict[str, str] | None = None,\n    on_duplicate: str = \"error\",\n):\n    self.source = source\n\n    # Validate exactly one mode\n    modes = sum(x is not None for x in [columns, sql, transform])\n    if modes != 1:\n        raise TimefenceConfigError(\n            \"Feature requires exactly one of 'columns', 'sql', or 'transform'. \"\n            f\"Got {modes} of them.\"\n        )\n\n    # Determine mode and normalize\n    if columns is not None:\n        self.mode = \"columns\"\n        if isinstance(columns, str):\n            self._columns = {columns: columns}\n        elif isinstance(columns, list):\n            self._columns = {c: c for c in columns}\n        else:\n            self._columns = dict(columns)\n        if not self._columns:\n            raise TimefenceConfigError(\n                \"Feature 'columns' cannot be empty. \"\n                \"Provide at least one column name.\"\n            )\n        self._sql_text = None\n        self._sql_path = None\n        self._transform = None\n    elif sql is not None:\n        self.mode = \"sql\"\n        if isinstance(sql, Path):\n            self._sql_path = sql\n            self._sql_text = sql.read_text()\n        else:\n            self._sql_path = None\n            self._sql_text = sql\n        self._columns = {}\n        self._transform = None\n    else:\n        self.mode = \"transform\"\n        self._transform = transform\n        self._columns = {}\n        self._sql_text = None\n        self._sql_path = None\n\n    # Derive name\n    if name is not None:\n        self.name = name\n    elif self.mode == \"columns\":\n        self.name = \"_\".join(self._columns.values())\n    elif (\n        self.mode == \"sql\"\n        and hasattr(self, \"_sql_path\")\n        and self._sql_path is not None\n    ):\n        self.name = self._sql_path.stem\n    elif self.mode == \"transform\":\n        self.name = transform.__name__  # type: ignore[union-attr]\n    else:\n        raise TimefenceConfigError(\n            \"Feature 'name' is required when using inline SQL. \"\n            \"Timefence cannot auto-derive a name from a SQL string.\"\n        )\n\n    self.embargo = parse_duration(embargo) or timedelta(0)\n    self.key_mapping = key_mapping or {}\n    self.on_duplicate = on_duplicate\n\n    if on_duplicate not in (\"error\", \"keep_any\"):\n        raise TimefenceConfigError(\n            f\"on_duplicate must be 'error' or 'keep_any', got '{on_duplicate}'.\"\n        )\n</code></pre>","path":["API Reference","Feature"],"tags":[]},{"location":"api/feature/#parameters","level":2,"title":"Parameters","text":"Parameter Type Description <code>source</code> <code>Source \\| SQLSource</code> The data source object. <code>columns</code> <code>str \\| list \\| dict \\| None</code> Mode 1: Select columns directly. Pass a dict to rename: <code>{\"source_col\": \"feature_col\"}</code>. <code>sql</code> <code>str \\| Path \\| None</code> Mode 2: SQL query or path to <code>.sql</code> file. Use <code>{source}</code> placeholder. <code>transform</code> <code>Callable \\| None</code> Mode 3: Python function <code>(conn, source_table) -&gt; DuckDBPyRelation</code>. Use <code>conn.sql(...)</code> to return a relation. <code>name</code> <code>str \\| None</code> Feature name. Auto-derived when possible. Required for inline SQL strings. <code>embargo</code> <code>str \\| timedelta \\| None</code> Computation lag buffer. See Embargo. <code>key_mapping</code> <code>dict[str, str] \\| None</code> Map label key names to source key names: <code>{\"user_id\": \"customer_id\"}</code>. <code>on_duplicate</code> <code>str</code> <code>\"error\"</code> (default) or <code>\"keep_any\"</code> when duplicate <code>(key, feature_time)</code> pairs exist.","path":["API Reference","Feature"],"tags":[]},{"location":"api/feature/#key_mapping","level":2,"title":"key_mapping","text":"<p>When label keys don't match source keys, use <code>key_mapping</code> to bridge them:</p> <pre><code># Labels use \"user_id\", but the source uses \"customer_id\"\nspend = timefence.Feature(\n    source=transactions,  # has column \"customer_id\"\n    columns=[\"amount\"],\n    key_mapping={\"user_id\": \"customer_id\"},\n)\n\n# Multi-key mapping\norders_feature = timefence.Feature(\n    source=orders,  # has \"cust_id\" and \"prod_id\"\n    columns=[\"quantity\"],\n    key_mapping={\"user_id\": \"cust_id\", \"product_id\": \"prod_id\"},\n)\n</code></pre> <p>The mapping format is <code>{label_key: source_key}</code>. During the join, Timefence rewrites <code>ON labels.user_id = source.customer_id</code> automatically.</p>","path":["API Reference","Feature"],"tags":[]},{"location":"api/feature/#feature-modes","level":2,"title":"Feature modes","text":"Column SelectionSQL (inline)SQL (file)Python Transform <pre><code>country = timefence.Feature(source=users, columns=[\"country\"])\n</code></pre> <pre><code>spend = timefence.Feature(\n    source=transactions,\n    sql=\"\"\"\n        SELECT user_id, created_at AS feature_time,\n        SUM(amount) OVER (\n            PARTITION BY user_id ORDER BY created_at\n            RANGE INTERVAL 30 DAYS PRECEDING\n        ) AS spend_30d\n        FROM {source}\n    \"\"\",\n    name=\"rolling_spend_30d\",\n    embargo=\"1d\",\n)\n</code></pre> <pre><code>spend = timefence.Feature(\n    source=transactions,\n    sql=Path(\"features/rolling_spend.sql\"),\n)\n</code></pre> <pre><code>def compute_score(conn, source_table):\n    return conn.sql(f\"\"\"\n        SELECT user_id, created_at AS feature_time,\n               raw_score * 2.5 AS adjusted_score\n        FROM {source_table}\n    \"\"\")\n\nscore = timefence.Feature(source=transactions, transform=compute_score)\n</code></pre>","path":["API Reference","Feature"],"tags":[]},{"location":"api/featureset/","level":1,"title":"FeatureSet","text":"<p>A named group of features for reuse across builds and audits.</p>","path":["API Reference","FeatureSet"],"tags":[]},{"location":"api/featureset/#timefence.FeatureSet","level":2,"title":"<code>FeatureSet(name, features)</code>","text":"<p>A named group of features for reuse across models.</p> <p>A FeatureSet is a flat list with a name. No nesting, no inheritance.</p> Source code in <code>src/timefence/core.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    features: Sequence[Feature],\n):\n    self.name = name\n    self.features = list(features)\n</code></pre>","path":["API Reference","FeatureSet"],"tags":[]},{"location":"api/featureset/#why-use-featureset","level":2,"title":"Why use FeatureSet","text":"<p>When you have a stable set of features that get reused across multiple models or experiments, group them into a <code>FeatureSet</code> instead of managing individual lists. This gives you:</p> <ul> <li>A single named reference for a group of features</li> <li>Version tracking — name the set <code>\"churn_features_v2\"</code> and swap it in one place</li> <li>Composability — combine FeatureSets with additional features in <code>build()</code> and <code>audit()</code></li> </ul>","path":["API Reference","FeatureSet"],"tags":[]},{"location":"api/featureset/#example","level":2,"title":"Example","text":"<pre><code>import timefence\n\n# Define individual features\nrolling_spend = timefence.Feature(source=transactions, columns=[\"amount\"], embargo=\"1d\", name=\"spend\")\nuser_country = timefence.Feature(source=users, columns=[\"country\"])\nlogin_count = timefence.Feature(source=logins, columns=[\"login_count\"])\n\n# Group into a named set\nbase_features = timefence.FeatureSet(\n    name=\"churn_features_v1\",\n    features=[rolling_spend, user_country, login_count],\n)\n\n# Use directly in build() — Timefence flattens it automatically\nresult = timefence.build(\n    labels=labels,\n    features=[base_features],\n    output=\"train.parquet\",\n)\n</code></pre>","path":["API Reference","FeatureSet"],"tags":[]},{"location":"api/featureset/#combining-featuresets-with-individual-features","level":2,"title":"Combining FeatureSets with individual features","text":"<p>You can mix FeatureSets and individual Features in any call:</p> <pre><code># Add an experimental feature alongside the base set\nresult = timefence.build(\n    labels=labels,\n    features=[base_features, new_experimental_feature],\n    output=\"train.parquet\",\n)\n\n# Audit with the same features\nreport = timefence.audit(\n    data=\"train.parquet\",\n    features=[base_features],\n    keys=[\"user_id\"],\n    label_time=\"label_time\",\n)\n</code></pre>","path":["API Reference","FeatureSet"],"tags":[]},{"location":"api/featureset/#iteration-and-length","level":2,"title":"Iteration and length","text":"<p>FeatureSet supports standard Python iteration:</p> <pre><code>print(len(base_features))  # 3\n\nfor feature in base_features:\n    print(feature.name)  # \"spend\", \"country\", \"login_count\"\n</code></pre>","path":["API Reference","FeatureSet"],"tags":[]},{"location":"api/featureset/#parameters","level":2,"title":"Parameters","text":"Parameter Type Description <code>name</code> <code>str</code> Human-readable name for this group. <code>features</code> <code>Sequence[Feature]</code> List of Feature objects. <p>Note</p> <p>FeatureSets are flat — they contain only <code>Feature</code> objects, not other FeatureSets. There is no nesting.</p>","path":["API Reference","FeatureSet"],"tags":[]},{"location":"api/labels/","level":1,"title":"Labels","text":"<p>Defines the prediction target: which entities, at what times, and what outcome.</p>","path":["API Reference","Labels"],"tags":[]},{"location":"api/labels/#timefence.Labels","level":2,"title":"<code>Labels(*, path=None, df=None, keys, label_time, target)</code>","text":"<p>Prediction targets with entity keys and event times.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path | None</code> <p>Path to the labels file.</p> <code>None</code> <code>df</code> <code>Any</code> <p>DataFrame with labels (mutually exclusive with path).</p> <code>None</code> <code>keys</code> <code>str | list[str]</code> <p>Column name(s) used as entity keys.</p> required <code>label_time</code> <code>str</code> <p>Column name containing the label event time.</p> required <code>target</code> <code>str | list[str]</code> <p>Column name(s) for the prediction target.</p> required Source code in <code>src/timefence/core.py</code> <pre><code>def __init__(\n    self,\n    *,\n    path: str | Path | None = None,\n    df: Any = None,\n    keys: str | list[str],\n    label_time: str,\n    target: str | list[str],\n):\n    if path is None and df is None:\n        raise TimefenceValidationError(\n            \"Labels requires either 'path' or 'df' parameter.\"\n        )\n    if path is not None and df is not None:\n        raise TimefenceValidationError(\n            \"Labels accepts either 'path' or 'df', not both.\"\n        )\n\n    self.path = Path(path) if path is not None else None\n    self.df = df\n    self.keys = _as_list(keys)\n    if not self.keys:\n        raise TimefenceValidationError(\n            \"Labels 'keys' cannot be empty. Provide at least one entity key column.\"\n        )\n    self.label_time = label_time\n    self.target = _as_list(target)\n    if not self.target:\n        raise TimefenceValidationError(\n            \"Labels 'target' cannot be empty. Provide at least one target column.\"\n        )\n</code></pre>","path":["API Reference","Labels"],"tags":[]},{"location":"api/labels/#parameters","level":2,"title":"Parameters","text":"Parameter Type Description <code>path</code> <code>str \\| Path \\| None</code> Path to labels file (Parquet). Mutually exclusive with <code>df</code>. <code>df</code> <code>Any \\| None</code> DataFrame, DuckDB relation, or any object with a compatible interface. Mutually exclusive with <code>path</code>. <code>keys</code> <code>str \\| list[str]</code> Entity key column(s). Must match the keys used in features. <code>label_time</code> <code>str</code> Column name for the label event timestamp. <code>target</code> <code>str \\| list[str]</code> Prediction target column(s) (e.g., <code>\"churned\"</code>).","path":["API Reference","Labels"],"tags":[]},{"location":"api/labels/#example","level":2,"title":"Example","text":"<pre><code>labels = timefence.Labels(\n    path=\"data/labels.parquet\",\n    keys=[\"user_id\"],\n    label_time=\"label_time\",\n    target=\"churned\",\n)\n</code></pre>","path":["API Reference","Labels"],"tags":[]},{"location":"api/source/","level":1,"title":"Source","text":"<p>Defines a historical data source (Parquet, CSV, or DataFrame).</p>","path":["API Reference","Source"],"tags":[]},{"location":"api/source/#timefence.Source","level":2,"title":"<code>Source(path=None, *, keys, timestamp, name=None, format=None, delimiter=',', timestamp_format=None, df=None)</code>","text":"<p>A table of historical data with timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path | None</code> <p>Path to the data file (Parquet or CSV).</p> <code>None</code> <code>keys</code> <code>str | list[str]</code> <p>Column name(s) used as entity keys.</p> required <code>timestamp</code> <code>str</code> <p>Column name containing the temporal key.</p> required <code>name</code> <code>str | None</code> <p>Human-readable name (defaults to filename stem).</p> <code>None</code> <code>format</code> <code>str | None</code> <p>File format (\"parquet\" or \"csv\"). Auto-detected from extension.</p> <code>None</code> <code>delimiter</code> <code>str</code> <p>CSV delimiter (only for CSV files).</p> <code>','</code> <code>timestamp_format</code> <code>str | None</code> <p>strftime format for parsing timestamp strings.</p> <code>None</code> Source code in <code>src/timefence/core.py</code> <pre><code>def __init__(\n    self,\n    path: str | Path | None = None,\n    *,\n    keys: str | list[str],\n    timestamp: str,\n    name: str | None = None,\n    format: str | None = None,\n    delimiter: str = \",\",\n    timestamp_format: str | None = None,\n    df: Any = None,\n):\n    if path is None and df is None:\n        raise TimefenceValidationError(\n            \"Source requires either 'path' or 'df' parameter.\"\n        )\n    if path is not None and df is not None:\n        raise TimefenceValidationError(\n            \"Source accepts either 'path' or 'df', not both.\"\n        )\n\n    self.path = Path(path) if path is not None else None\n    self.df = df\n    self.keys = _as_list(keys)\n    if not self.keys:\n        raise TimefenceValidationError(\n            \"Source 'keys' cannot be empty. Provide at least one entity key column.\"\n        )\n    self.timestamp = timestamp\n    self.name = name or (self.path.stem if self.path else \"dataframe\")\n    self.delimiter = delimiter\n    self.timestamp_format = timestamp_format\n\n    if format is not None:\n        self.format = format\n    elif self.path is not None:\n        ext = self.path.suffix.lower()\n        if ext in (\".parquet\", \".pq\"):\n            self.format = \"parquet\"\n        elif ext == \".csv\":\n            self.format = \"csv\"\n        else:\n            raise TimefenceValidationError(\n                f\"Cannot auto-detect format for '{self.path}'. \"\n                \"Specify format='parquet' or format='csv'.\"\n            )\n    else:\n        self.format = \"arrow\"\n</code></pre>","path":["API Reference","Source"],"tags":[]},{"location":"api/source/#parameters","level":2,"title":"Parameters","text":"Parameter Type Description <code>path</code> <code>str \\| Path \\| None</code> Path to the data file. Mutually exclusive with <code>df</code>. <code>keys</code> <code>str \\| list[str]</code> Column name(s) representing the entity (e.g., <code>\"user_id\"</code>). <code>timestamp</code> <code>str</code> Column name containing the valid-at timestamp. <code>name</code> <code>str \\| None</code> Human-readable name. Defaults to filename stem. <code>format</code> <code>str \\| None</code> <code>\"parquet\"</code> or <code>\"csv\"</code>. Auto-detected from file extension. <code>delimiter</code> <code>str</code> CSV delimiter. Default: <code>\",\"</code>. <code>timestamp_format</code> <code>str \\| None</code> Optional strftime format for parsing timestamps (CSV only). <code>df</code> <code>Any \\| None</code> Pass a DataFrame, DuckDB relation, or any object with a compatible interface instead of a file path. <p>Mutual exclusivity</p> <p>Provide exactly one of <code>path</code> or <code>df</code>. Passing both raises <code>TimefenceValidationError</code>. Passing neither also raises an error.</p>","path":["API Reference","Source"],"tags":[]},{"location":"api/source/#examples","level":2,"title":"Examples","text":"<pre><code># Parquet source\ntransactions = timefence.Source(\n    path=\"data/transactions.parquet\",\n    keys=[\"user_id\"],\n    timestamp=\"created_at\",\n)\n\n# CSV source\nevents = timefence.Source(\n    path=\"data/events.csv\",\n    keys=[\"user_id\"],\n    timestamp=\"event_time\",\n    format=\"csv\",\n    delimiter=\",\",\n)\n\n# DataFrame source\ndf_source = timefence.Source(\n    df=my_dataframe,\n    keys=[\"user_id\"],\n    timestamp=\"created_at\",\n)\n\n# Multi-key source\norders = timefence.Source(\n    path=\"data/orders.parquet\",\n    keys=[\"user_id\", \"product_id\"],\n    timestamp=\"order_time\",\n)\n</code></pre>","path":["API Reference","Source"],"tags":[]},{"location":"api/source/#convenience-aliases","level":2,"title":"Convenience aliases","text":"<pre><code>transactions = timefence.ParquetSource(\"data/tx.parquet\", keys=\"user_id\", timestamp=\"ts\")\nevents = timefence.CSVSource(\"data/events.csv\", keys=\"user_id\", timestamp=\"ts\")\n</code></pre> <p>These are thin wrappers that set <code>format</code> automatically.</p>","path":["API Reference","Source"],"tags":[]},{"location":"api/source/#sqlsource","level":2,"title":"SQLSource","text":"<p>Define a source via a SQL query against DuckDB. Use this when your data requires pre-processing (joins, filters, aggregations) before it can serve as a feature source.</p>","path":["API Reference","Source"],"tags":[]},{"location":"api/source/#timefence.SQLSource","level":2,"title":"<code>SQLSource(query, *, keys, timestamp, name, connection=None)</code>","text":"<p>A source defined by a SQL query against DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL query string. Can use read_parquet/read_csv directly.</p> required <code>keys</code> <code>str | list[str]</code> <p>Column name(s) used as entity keys.</p> required <code>timestamp</code> <code>str</code> <p>Column name containing the temporal key.</p> required <code>name</code> <code>str</code> <p>Human-readable name.</p> required <code>connection</code> <code>str | None</code> <p>Path to a DuckDB database file (optional, uses in-memory by default).</p> <code>None</code> Source code in <code>src/timefence/core.py</code> <pre><code>def __init__(\n    self,\n    query: str,\n    *,\n    keys: str | list[str],\n    timestamp: str,\n    name: str,\n    connection: str | None = None,\n):\n    self.query = query\n    self.keys = _as_list(keys)\n    self.timestamp = timestamp\n    self.name = name\n    self.connection = connection\n    self.path = None\n    self.df = None\n    self.format = \"sql\"\n</code></pre>","path":["API Reference","Source"],"tags":[]},{"location":"api/source/#parameters_1","level":3,"title":"Parameters","text":"Parameter Type Description <code>query</code> <code>str</code> SQL query string. Can use <code>read_parquet()</code> / <code>read_csv()</code> directly. <code>keys</code> <code>str \\| list[str]</code> Column name(s) used as entity keys. <code>timestamp</code> <code>str</code> Column name containing the temporal key. <code>name</code> <code>str</code> Human-readable name (required — cannot be auto-derived from a query). <code>connection</code> <code>str \\| None</code> Path to a DuckDB database file. Default: <code>None</code> (in-memory).","path":["API Reference","Source"],"tags":[]},{"location":"api/source/#examples_1","level":3,"title":"Examples","text":"<pre><code>import timefence\n\n# Query across multiple parquet files\ncombined = timefence.SQLSource(\n    query=\"\"\"\n        SELECT user_id, event_time, amount\n        FROM read_parquet('data/transactions_*.parquet')\n        WHERE amount &gt; 0\n    \"\"\",\n    keys=[\"user_id\"],\n    timestamp=\"event_time\",\n    name=\"positive_transactions\",\n)\n\n# Pre-aggregate before using as a feature source\ndaily_spend = timefence.SQLSource(\n    query=\"\"\"\n        SELECT user_id, DATE_TRUNC('day', created_at) AS day,\n               SUM(amount) AS daily_total\n        FROM read_parquet('data/transactions.parquet')\n        GROUP BY user_id, DATE_TRUNC('day', created_at)\n    \"\"\",\n    keys=[\"user_id\"],\n    timestamp=\"day\",\n    name=\"daily_spend\",\n)\n\n# Join two files before use\nenriched = timefence.SQLSource(\n    query=\"\"\"\n        SELECT t.user_id, t.created_at, t.amount, u.country\n        FROM read_parquet('data/transactions.parquet') t\n        JOIN read_parquet('data/users.parquet') u\n          ON t.user_id = u.user_id\n    \"\"\",\n    keys=[\"user_id\"],\n    timestamp=\"created_at\",\n    name=\"enriched_transactions\",\n)\n\n# Use with an existing DuckDB database\nwarehouse = timefence.SQLSource(\n    query=\"SELECT user_id, updated_at, score FROM user_scores\",\n    keys=[\"user_id\"],\n    timestamp=\"updated_at\",\n    name=\"user_scores\",\n    connection=\"analytics.duckdb\",\n)\n</code></pre>","path":["API Reference","Source"],"tags":[]},{"location":"api/source/#when-to-use-sqlsource","level":3,"title":"When to use SQLSource","text":"Scenario Use Single Parquet or CSV file <code>Source</code> (simpler) Glob patterns (<code>*.parquet</code>) <code>SQLSource</code> Pre-filtering rows <code>SQLSource</code> Joining multiple tables <code>SQLSource</code> Existing DuckDB database <code>SQLSource</code> with <code>connection</code> DataFrame in memory <code>Source</code> with <code>df</code>","path":["API Reference","Source"],"tags":[]},{"location":"api/store/","level":1,"title":"Store","text":"<p>Local directory for build tracking and feature-level caching.</p>","path":["API Reference","Store"],"tags":[]},{"location":"api/store/#timefence.Store","level":2,"title":"<code>Store(path=DEFAULT_STORE_PATH)</code>","text":"<p>Local directory that tracks builds and manifests.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Directory path for the store (default: \".timefence\").</p> <code>DEFAULT_STORE_PATH</code> Source code in <code>src/timefence/store.py</code> <pre><code>def __init__(self, path: str | Path = DEFAULT_STORE_PATH):\n    self.path = Path(path)\n    self._ensure_dirs()\n</code></pre>","path":["API Reference","Store"],"tags":[]},{"location":"api/store/#timefence.Store.save_build","level":3,"title":"<code>save_build(manifest)</code>","text":"<p>Save a build manifest and create a symlink to the output.</p> Source code in <code>src/timefence/store.py</code> <pre><code>def save_build(self, manifest: dict[str, Any]) -&gt; Path:\n    \"\"\"Save a build manifest and create a symlink to the output.\"\"\"\n    build_id = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n    build_dir = self.path / \"builds\" / build_id\n    build_dir.mkdir(parents=True, exist_ok=True)\n\n    manifest[\"build_id\"] = build_id\n    manifest_path = build_dir / \"build.json\"\n    manifest_path.write_text(json.dumps(manifest, indent=2, default=str))\n\n    # Create symlink to output file\n    output_path = manifest.get(\"output\", {}).get(\"path\")\n    if output_path:\n        output_abs = Path(output_path).resolve()\n        if output_abs.exists():\n            import contextlib\n\n            link_path = build_dir / output_abs.name\n            with contextlib.suppress(OSError):\n                link_path.symlink_to(output_abs)\n\n    return manifest_path\n</code></pre>","path":["API Reference","Store"],"tags":[]},{"location":"api/store/#timefence.Store.list_builds","level":3,"title":"<code>list_builds()</code>","text":"<p>List all builds in the store, newest first.</p> Source code in <code>src/timefence/store.py</code> <pre><code>def list_builds(self) -&gt; list[dict[str, Any]]:\n    \"\"\"List all builds in the store, newest first.\"\"\"\n    builds_dir = self.path / \"builds\"\n    if not builds_dir.exists():\n        return []\n\n    builds = []\n    for build_dir in sorted(builds_dir.iterdir(), reverse=True):\n        manifest_path = build_dir / \"build.json\"\n        if manifest_path.exists():\n            builds.append(json.loads(manifest_path.read_text()))\n    return builds\n</code></pre>","path":["API Reference","Store"],"tags":[]},{"location":"api/store/#timefence.Store.get_build","level":3,"title":"<code>get_build(build_id)</code>","text":"<p>Get a specific build manifest by ID.</p> Source code in <code>src/timefence/store.py</code> <pre><code>def get_build(self, build_id: str) -&gt; dict[str, Any] | None:\n    \"\"\"Get a specific build manifest by ID.\"\"\"\n    manifest_path = self.path / \"builds\" / build_id / \"build.json\"\n    if manifest_path.exists():\n        return json.loads(manifest_path.read_text())\n    return None\n</code></pre>","path":["API Reference","Store"],"tags":[]},{"location":"api/store/#timefence.Store.content_hash","level":3,"title":"<code>content_hash(path)</code>  <code>staticmethod</code>","text":"<p>Compute full SHA-256 content hash of a file.</p> Source code in <code>src/timefence/store.py</code> <pre><code>@staticmethod\ndef content_hash(path: str | Path) -&gt; str:\n    \"\"\"Compute full SHA-256 content hash of a file.\"\"\"\n    h = hashlib.sha256()\n    with open(path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(8192), b\"\"):\n            h.update(chunk)\n    return f\"sha256:{h.hexdigest()}\"\n</code></pre>","path":["API Reference","Store"],"tags":[]},{"location":"api/store/#timefence.Store.cached_content_hash","level":3,"title":"<code>cached_content_hash(path)</code>","text":"<p>Content hash with (path, size, mtime_ns) caching for speed.</p> Source code in <code>src/timefence/store.py</code> <pre><code>def cached_content_hash(self, path: str | Path) -&gt; str:\n    \"\"\"Content hash with (path, size, mtime_ns) caching for speed.\"\"\"\n    path = Path(path).resolve()\n    cache_file = self.path / \"cache\" / \"hashes.json\"\n\n    cache: dict[str, str] = {}\n    if cache_file.exists():\n        cache = json.loads(cache_file.read_text())\n\n    stat = path.stat()\n    cache_key = f\"{path}:{stat.st_size}:{stat.st_mtime_ns}\"\n\n    if cache_key in cache:\n        return cache[cache_key]\n\n    content_hash = self.content_hash(path)\n    cache[cache_key] = content_hash\n    cache_file.write_text(json.dumps(cache, indent=2))\n    return content_hash\n</code></pre>","path":["API Reference","Store"],"tags":[]},{"location":"api/store/#timefence.Store.feature_cache_key","level":3,"title":"<code>feature_cache_key(definition_hash, source_content_hash, embargo)</code>","text":"<p>Compute a cache key for a single feature computation.</p> Source code in <code>src/timefence/store.py</code> <pre><code>def feature_cache_key(\n    self,\n    definition_hash: str,\n    source_content_hash: str | None,\n    embargo: str | None,\n) -&gt; str:\n    \"\"\"Compute a cache key for a single feature computation.\"\"\"\n    key_input = (\n        f\"{definition_hash}:{source_content_hash or ''}:\"\n        f\"{embargo or '0d'}:{__version__}\"\n    )\n    return hashlib.sha256(key_input.encode()).hexdigest()[:CACHE_KEY_LENGTH]\n</code></pre>","path":["API Reference","Store"],"tags":[]},{"location":"api/store/#timefence.Store.feature_cache_path","level":3,"title":"<code>feature_cache_path(feature_name, cache_key)</code>","text":"<p>Path where a cached feature table would be stored.</p> Source code in <code>src/timefence/store.py</code> <pre><code>def feature_cache_path(self, feature_name: str, cache_key: str) -&gt; Path:\n    \"\"\"Path where a cached feature table would be stored.\"\"\"\n    return self.path / \"cache\" / \"features\" / f\"{feature_name}__{cache_key}.parquet\"\n</code></pre>","path":["API Reference","Store"],"tags":[]},{"location":"api/store/#timefence.Store.build_cache_key","level":3,"title":"<code>build_cache_key(label_content_hash, feature_cache_keys, max_lookback, max_staleness, join_mode, on_missing)</code>","text":"<p>Compute a cache key for an entire build.</p> Source code in <code>src/timefence/store.py</code> <pre><code>def build_cache_key(\n    self,\n    label_content_hash: str | None,\n    feature_cache_keys: list[str],\n    max_lookback: str | None,\n    max_staleness: str | None,\n    join_mode: str,\n    on_missing: str,\n) -&gt; str:\n    \"\"\"Compute a cache key for an entire build.\"\"\"\n    key_input = (\n        f\"{label_content_hash or ''}:\"\n        f\"{sorted(feature_cache_keys)}:\"\n        f\"{max_lookback}:{max_staleness}:{join_mode}:{on_missing}\"\n    )\n    return hashlib.sha256(key_input.encode()).hexdigest()[:CACHE_KEY_LENGTH]\n</code></pre>","path":["API Reference","Store"],"tags":[]},{"location":"api/store/#timefence.Store.find_cached_build","level":3,"title":"<code>find_cached_build(build_cache_key)</code>","text":"<p>Find a previous build matching this cache key.</p> Source code in <code>src/timefence/store.py</code> <pre><code>def find_cached_build(self, build_cache_key: str) -&gt; dict[str, Any] | None:\n    \"\"\"Find a previous build matching this cache key.\"\"\"\n    for build in self.list_builds():\n        if build.get(\"build_cache_key\") == build_cache_key:\n            output_path = build.get(\"output\", {}).get(\"path\")\n            if output_path and Path(output_path).exists():\n                return build\n    return None\n</code></pre>","path":["API Reference","Store"],"tags":[]},{"location":"api/store/#how-it-works","level":2,"title":"How it works","text":"<p>When you pass a <code>Store</code> to <code>build()</code>, Timefence:</p> <ol> <li>Hashes inputs — content hash (SHA-256) of source files, feature definitions, embargo values, and build parameters.</li> <li>Checks feature cache — if a feature's inputs haven't changed, the cached intermediate table is loaded instead of recomputed.</li> <li>Checks build cache — if all features + labels + parameters match a previous build, the entire result is returned immediately.</li> <li>Saves the manifest — every build writes a JSON manifest with full provenance to <code>.timefence/builds/&lt;build_id&gt;/build.json</code>.</li> </ol>","path":["API Reference","Store"],"tags":[]},{"location":"api/store/#directory-structure","level":2,"title":"Directory structure","text":"<pre><code>.timefence/\n├── builds/\n│   ├── 20240315T120000Z/\n│   │   ├── build.json        # Full build manifest\n│   │   └── train.parquet     # Symlink to output file\n│   └── 20240316T090000Z/\n│       └── build.json\n└── cache/\n    ├── hashes.json            # Content hash cache (path:size:mtime → hash)\n    └── features/\n        ├── rolling_spend__a1b2c3d4.parquet\n        └── user_country__e5f6g7h8.parquet\n</code></pre>","path":["API Reference","Store"],"tags":[]},{"location":"api/store/#example","level":2,"title":"Example","text":"<pre><code>import timefence\n\nstore = timefence.Store(\".timefence\")\n\n# First build: computes everything, caches results\nresult = timefence.build(\n    labels=labels,\n    features=[rolling_spend, user_country],\n    output=\"train.parquet\",\n    store=store,\n)\n\n# Second build (same inputs): returns cached result in milliseconds\nresult = timefence.build(\n    labels=labels,\n    features=[rolling_spend, user_country],\n    output=\"train.parquet\",\n    store=store,\n)\n\n# Add a new feature: only the new one is computed, others loaded from cache\nresult = timefence.build(\n    labels=labels,\n    features=[rolling_spend, user_country, login_count],\n    output=\"train.parquet\",\n    store=store,\n)\n</code></pre>","path":["API Reference","Store"],"tags":[]},{"location":"api/store/#build-history","level":2,"title":"Build history","text":"<pre><code>store = timefence.Store(\".timefence\")\n\n# List all past builds (newest first)\nbuilds = store.list_builds()\nfor b in builds:\n    print(f\"{b['build_id']}  {b['output']['row_count']} rows  {b['duration_seconds']:.1f}s\")\n\n# Get a specific build by ID\nmanifest = store.get_build(\"20240315T120000Z\")\nif manifest:\n    print(manifest[\"features\"])    # Feature-level stats\n    print(manifest[\"parameters\"])  # max_lookback, join, etc.\n    print(manifest[\"audit\"])       # Post-build audit result\n</code></pre>","path":["API Reference","Store"],"tags":[]},{"location":"api/store/#cache-invalidation","level":2,"title":"Cache invalidation","text":"<p>Cache keys are recomputed from content hashes on every build. The cache is automatically invalidated when:</p> Change What happens Source data changes (any byte) Feature recomputed from scratch Feature definition changes (SQL, columns, transform) Feature recomputed Embargo value changes Feature recomputed Timefence version changes Feature recomputed Labels change Build recomputed (features may still be cached) <code>max_lookback</code> / <code>max_staleness</code> / <code>join</code> / <code>on_missing</code> change Build recomputed <p>To manually clear the cache, delete the <code>.timefence/cache/</code> directory:</p> <pre><code>rm -rf .timefence/cache/\n</code></pre>","path":["API Reference","Store"],"tags":[]},{"location":"api/store/#methods","level":2,"title":"Methods","text":"Method Returns Description <code>.save_build(manifest)</code> <code>Path</code> Save build manifest, return manifest path. <code>.list_builds()</code> <code>list[dict]</code> List all builds (newest first). <code>.get_build(build_id)</code> <code>dict \\| None</code> Get a specific build manifest by ID. <code>.content_hash(path)</code> <code>str</code> Compute SHA-256 hash of a file (e.g., <code>\"sha256:abc123...\"</code>).","path":["API Reference","Store"],"tags":[]},{"location":"api/store/#parameters","level":2,"title":"Parameters","text":"Parameter Type Default Description <code>path</code> <code>str \\| Path</code> <code>\".timefence\"</code> Directory path for the store. <p>Tip</p> <p>Add <code>.timefence/</code> to your <code>.gitignore</code>. The store is local-only and should not be committed.</p>","path":["API Reference","Store"],"tags":[]},{"location":"cli/","level":1,"title":"CLI Reference","text":"<p>Timefence provides a full command-line interface built on Click with Rich terminal output.</p>","path":["CLI Reference"],"tags":[]},{"location":"cli/#global-options","level":2,"title":"Global Options","text":"<p>These flags apply to all commands when placed before the subcommand name.</p> <pre><code>timefence --version             # Show version and exit\ntimefence -v &lt;command&gt;          # Verbose: show generated SQL and details\ntimefence --debug &lt;command&gt;     # Debug: full output including DuckDB internals\n</code></pre> Flag Description <code>--version</code> Show version and exit <code>-v, --verbose</code> Show generated SQL and details <code>--debug</code> Show full debug output including DuckDB internals","path":["CLI Reference"],"tags":[]},{"location":"cli/#commands","level":2,"title":"Commands","text":"Command Description <code>audit</code> Scan a dataset for temporal leakage <code>build</code> Build a point-in-time correct training set <code>explain</code> Preview join logic without executing <code>diff</code> Compare two datasets <code>inspect</code> Analyze a data file <code>quickstart</code> Generate a sample project <code>catalog</code> List all features <code>doctor</code> Diagnose project setup <code>init</code> Initialize a project","path":["CLI Reference"],"tags":[]},{"location":"cli/audit/","level":1,"title":"timefence audit","text":"<p>Scan a dataset for temporal leakage.</p>","path":["CLI Reference","timefence audit"],"tags":[]},{"location":"cli/audit/#usage","level":2,"title":"Usage","text":"<pre><code># Basic audit\ntimefence audit data/train.parquet\n\n# With explicit options\ntimefence audit data/train.parquet \\\n  --features features.py \\\n  --keys user_id \\\n  --label-time label_time\n\n# CI mode: exit code 1 if leakage found\ntimefence audit data/train.parquet --strict\n\n# Export reports\ntimefence audit data/train.parquet --html report.html\ntimefence audit data/train.parquet --json\n</code></pre>","path":["CLI Reference","timefence audit"],"tags":[]},{"location":"cli/audit/#options","level":2,"title":"Options","text":"Option Description <code>data</code> Positional. Path to the dataset file. <code>--features</code> Path to Python file with feature definitions. <code>--keys</code> Key column(s), comma-separated. <code>--label-time</code> Label time column name. Default: <code>\"label_time\"</code>. <code>--strict</code> Exit with code 1 if leakage found (for CI/CD). <code>--html FILE</code> Export interactive HTML report. <code>--json</code> Output as JSON.","path":["CLI Reference","timefence audit"],"tags":[]},{"location":"cli/build/","level":1,"title":"timefence build","text":"<p>Build a point-in-time correct training set. Uses <code>timefence.yaml</code> defaults if available.</p>","path":["CLI Reference","timefence build"],"tags":[]},{"location":"cli/build/#usage","level":2,"title":"Usage","text":"<pre><code># Basic build\ntimefence build \\\n  --labels data/labels.parquet \\\n  --features features.py \\\n  -o train.parquet\n\n# With all options\ntimefence build \\\n  --labels data/labels.parquet \\\n  --features features.py \\\n  -o train.parquet \\\n  --max-lookback 365d \\\n  --max-staleness 30d \\\n  --on-missing null \\\n  --join-mode strict\n\n# Time-based splits\ntimefence build \\\n  --labels data/labels.parquet \\\n  --features features.py \\\n  -o train.parquet \\\n  --split train:2023-01-01:2024-01-01 \\\n  --split test:2024-01-01:2025-01-01\n\n# Dry run (preview plan without executing)\ntimefence build \\\n  --labels data/labels.parquet \\\n  --features features.py \\\n  -o train.parquet \\\n  --dry-run\n</code></pre>","path":["CLI Reference","timefence build"],"tags":[]},{"location":"cli/build/#options","level":2,"title":"Options","text":"Option Description <code>--labels</code> Path to labels file (required unless in config). <code>--features</code> Path to features Python file (required unless in config). <code>-o, --output</code> Output path for training set (required). <code>--max-lookback</code> Maximum lookback window (e.g., <code>\"365d\"</code>). Default: <code>365d</code>. <code>--max-staleness</code> Maximum feature staleness. <code>--on-missing</code> <code>null</code> or <code>skip</code>. Default: <code>null</code>. <code>--join-mode</code> <code>strict</code> or <code>inclusive</code>. Default: <code>strict</code>. <code>--split</code> Time split: <code>name:start:end</code> (repeatable). <code>--dry-run</code> Preview the join plan without executing queries. <code>--flatten</code> Strip the <code>{feature_name}__</code> prefix from output columns. For example, <code>spend__amount</code> becomes <code>amount</code>. <code>--json</code> Output the full build manifest as JSON instead of the Rich terminal display.","path":["CLI Reference","timefence build"],"tags":[]},{"location":"cli/catalog/","level":1,"title":"timefence catalog","text":"<p>List all features defined in a project.</p>","path":["CLI Reference","timefence catalog"],"tags":[]},{"location":"cli/catalog/#usage","level":2,"title":"Usage","text":"<pre><code>timefence catalog --features features.py\ntimefence catalog --features features.py --json\n</code></pre>","path":["CLI Reference","timefence catalog"],"tags":[]},{"location":"cli/catalog/#options","level":2,"title":"Options","text":"Option Description <code>--features</code> Path to features Python file. <code>--json</code> Output as JSON. <p>Output shows Name, Source, Keys, Embargo, and Mode for each feature.</p>","path":["CLI Reference","timefence catalog"],"tags":[]},{"location":"cli/diff/","level":1,"title":"timefence diff","text":"<p>Compare two datasets for value changes or schema drift.</p>","path":["CLI Reference","timefence diff"],"tags":[]},{"location":"cli/diff/#usage","level":2,"title":"Usage","text":"<pre><code>timefence diff train_v1.parquet train_v2.parquet \\\n  --keys user_id \\\n  --label-time label_time\n\n# Custom tolerance\ntimefence diff v1.parquet v2.parquet \\\n  --keys user_id \\\n  --label-time label_time \\\n  --atol 0.01 \\\n  --rtol 0.001\n\n# JSON output\ntimefence diff v1.parquet v2.parquet \\\n  --keys user_id \\\n  --label-time label_time \\\n  --json\n</code></pre>","path":["CLI Reference","timefence diff"],"tags":[]},{"location":"cli/diff/#options","level":2,"title":"Options","text":"Option Description <code>old_path</code> Positional. Path to first dataset. <code>new_path</code> Positional. Path to second dataset. <code>--keys</code> Key column(s), comma-separated (required). <code>--label-time</code> Label time column (required). <code>--atol</code> Absolute tolerance for numeric comparison (default: <code>1e-10</code>). <code>--rtol</code> Relative tolerance for numeric comparison (default: <code>1e-7</code>). <code>--json</code> Output as JSON.","path":["CLI Reference","timefence diff"],"tags":[]},{"location":"cli/doctor/","level":1,"title":"timefence doctor","text":"<p>Diagnose project setup and common issues.</p>","path":["CLI Reference","timefence doctor"],"tags":[]},{"location":"cli/doctor/#usage","level":2,"title":"Usage","text":"<pre><code>timefence doctor\ntimefence doctor --json\n</code></pre>","path":["CLI Reference","timefence doctor"],"tags":[]},{"location":"cli/doctor/#options","level":2,"title":"Options","text":"Option Description <code>--json</code> Output as JSON.","path":["CLI Reference","timefence doctor"],"tags":[]},{"location":"cli/doctor/#checks-performed","level":2,"title":"Checks performed","text":"<ul> <li>Config file presence and validity</li> <li>DuckDB installation and version</li> <li>Feature file validity</li> <li>Source file accessibility</li> <li>Label schema compatibility</li> <li>Duplicate (key, timestamp) combinations</li> <li>Column name conflicts</li> </ul>","path":["CLI Reference","timefence doctor"],"tags":[]},{"location":"cli/explain/","level":1,"title":"timefence explain","text":"<p>Preview join logic without executing any queries.</p>","path":["CLI Reference","timefence explain"],"tags":[]},{"location":"cli/explain/#usage","level":2,"title":"Usage","text":"<pre><code># Full explain\ntimefence explain \\\n  --labels data/labels.parquet \\\n  --features features.py\n\n# Single feature\ntimefence explain --features features.py:rolling_spend_30d\n\n# JSON output\ntimefence explain --features features.py --json\n</code></pre>","path":["CLI Reference","timefence explain"],"tags":[]},{"location":"cli/explain/#options","level":2,"title":"Options","text":"Option Description <code>--labels</code> Path to labels file. <code>--features</code> Path to features Python file. Append <code>:name</code> for single feature. <code>--max-lookback</code> Maximum lookback window. Default: <code>365d</code>. <code>--join-mode</code> <code>strict</code> or <code>inclusive</code>. Default: <code>strict</code>. <code>--json</code> Output as JSON.","path":["CLI Reference","timefence explain"],"tags":[]},{"location":"cli/init/","level":1,"title":"timefence init","text":"<p>Initialize a project with a <code>timefence.yaml</code> config file.</p>","path":["CLI Reference","timefence init"],"tags":[]},{"location":"cli/init/#usage","level":2,"title":"Usage","text":"<pre><code># Current directory\ntimefence init\n\n# Specific directory\ntimefence init my-project/\n</code></pre>","path":["CLI Reference","timefence init"],"tags":[]},{"location":"cli/init/#options","level":2,"title":"Options","text":"Option Description <code>path</code> Positional (optional). Directory to create config in. Default: current directory.","path":["CLI Reference","timefence init"],"tags":[]},{"location":"cli/inspect/","level":1,"title":"timefence inspect","text":"<p>Analyze a data file and suggest which columns are keys and timestamps.</p>","path":["CLI Reference","timefence inspect"],"tags":[]},{"location":"cli/inspect/#usage","level":2,"title":"Usage","text":"<pre><code>timefence inspect data/transactions.parquet\ntimefence inspect data/events.csv --json\n</code></pre>","path":["CLI Reference","timefence inspect"],"tags":[]},{"location":"cli/inspect/#options","level":2,"title":"Options","text":"Option Description <code>path</code> Positional. Path to data file (Parquet or CSV). <code>--json</code> Output as JSON. <p>Output includes column names, types, uniqueness percentage, and auto-detected key/timestamp suggestions.</p>","path":["CLI Reference","timefence inspect"],"tags":[]},{"location":"cli/quickstart/","level":1,"title":"timefence quickstart","text":"<p>Generate a self-contained example project with synthetic data and leakage scenarios.</p>","path":["CLI Reference","timefence quickstart"],"tags":[]},{"location":"cli/quickstart/#usage","level":2,"title":"Usage","text":"<pre><code># Default churn example\ntimefence quickstart churn-example\n\n# Minimal version\ntimefence quickstart myproject --minimal\n</code></pre>","path":["CLI Reference","timefence quickstart"],"tags":[]},{"location":"cli/quickstart/#options","level":2,"title":"Options","text":"Option Description <code>project_name</code> Positional. Directory name to create. Optional; defaults to <code>\"churn-example\"</code>. <code>--template</code> Example template. Default: <code>\"churn\"</code>. Accepted but currently has no effect. <code>--minimal</code> Generate a smaller example.","path":["CLI Reference","timefence quickstart"],"tags":[]},{"location":"concepts/caching/","level":1,"title":"Caching &amp; Store","text":"<p>Timefence can track builds and cache intermediate results using a local <code>Store</code>.</p>","path":["Core Concepts","Caching &amp; Store"],"tags":[]},{"location":"concepts/caching/#usage","level":2,"title":"Usage","text":"<pre><code>store = timefence.Store(\".timefence\")\n\nresult = timefence.build(\n    labels=labels,\n    features=[rolling_spend],\n    output=\"train.parquet\",\n    store=store,\n)\n</code></pre>","path":["Core Concepts","Caching &amp; Store"],"tags":[]},{"location":"concepts/caching/#how-it-works","level":2,"title":"How it works","text":"<p>Cache keys are computed from content hashes of:</p> <ul> <li>Source files (SHA-256 of file contents)</li> <li>Feature definitions (SQL, columns, embargo, etc.)</li> <li>Build parameters (max_lookback, max_staleness, join mode)</li> </ul> <p>If nothing has changed, subsequent builds return cached results in milliseconds.</p>","path":["Core Concepts","Caching &amp; Store"],"tags":[]},{"location":"concepts/caching/#feature-level-caching","level":2,"title":"Feature-level caching","text":"<p>Individual features are cached independently. If you add a new feature to an existing build, only the new feature is computed — the others are loaded from cache.</p>","path":["Core Concepts","Caching &amp; Store"],"tags":[]},{"location":"concepts/caching/#build-history","level":2,"title":"Build history","text":"<pre><code>store = timefence.Store(\".timefence\")\n\n# List past builds\nbuilds = store.list_builds()  # Newest first\n\n# Get a specific build\nmanifest = store.get_build(\"abc123\")\n</code></pre> <p>See the Store API reference for the full interface.</p>","path":["Core Concepts","Caching &amp; Store"],"tags":[]},{"location":"concepts/embargo/","level":1,"title":"Embargo","text":"<p>Real-world ML pipelines have latency: data arrives late, ETL jobs run on schedules, and features take time to compute. The embargo parameter models this lag.</p>","path":["Core Concepts","Embargo"],"tags":[]},{"location":"concepts/embargo/#usage","level":2,"title":"Usage","text":"<pre><code>rolling_spend = timefence.Feature(\n    source=transactions,\n    sql=\"SELECT ...\",\n    embargo=\"1d\"  # Feature available 1 day after event\n)\n</code></pre>","path":["Core Concepts","Embargo"],"tags":[]},{"location":"concepts/embargo/#how-it-works","level":2,"title":"How it works","text":"<p>With <code>embargo=\"1d\"</code>, a feature recorded at <code>2024-03-15 10:00</code> is only eligible for labels at <code>2024-03-16 10:00</code> or later. This prevents optimistic leakage from features that wouldn't actually be available in production.</p> <p>The full temporal constraint becomes:</p> <pre><code>feature_time &lt; label_time - embargo\n</code></pre>","path":["Core Concepts","Embargo"],"tags":[]},{"location":"concepts/embargo/#when-to-use-embargo","level":2,"title":"When to use embargo","text":"Scenario Recommended Embargo Real-time features <code>\"0d\"</code> (no embargo) Daily ETL pipeline <code>\"1d\"</code> Weekly batch features <code>\"7d\"</code> Monthly aggregates <code>\"30d\"</code> <p>Tip</p> <p>When in doubt, set embargo to match your production pipeline's worst-case latency. It's better to be conservative (larger embargo) than to train on features that wouldn't actually be available at prediction time.</p>","path":["Core Concepts","Embargo"],"tags":[]},{"location":"concepts/embargo/#duration-format","level":2,"title":"Duration format","text":"<p>Accepted formats: <code>\"30d\"</code>, <code>\"1d12h\"</code>, <code>\"6h\"</code>, <code>\"30m\"</code>, <code>\"15s\"</code>.</p> <p>See Duration Format for the full specification.</p>","path":["Core Concepts","Embargo"],"tags":[]},{"location":"concepts/join-logic/","level":1,"title":"Join Logic","text":"<p>Timefence performs an as-of join (also called a point-in-time join) for each feature. For each label row, it finds the most recent feature value that satisfies the temporal constraint.</p>","path":["Core Concepts","Join Logic"],"tags":[]},{"location":"concepts/join-logic/#join-strategies","level":2,"title":"Join strategies","text":"<p>Timefence automatically selects the best SQL strategy:</p> Strategy When Used Performance ASOF JOIN No embargo (fast path) Fastest — native DuckDB operator ROW_NUMBER With embargo or complex constraints Universal fallback, always correct <p>If ASOF JOIN fails for any reason, Timefence automatically falls back to ROW_NUMBER with a warning.</p>","path":["Core Concepts","Join Logic"],"tags":[]},{"location":"concepts/join-logic/#additional-constraints","level":2,"title":"Additional constraints","text":"","path":["Core Concepts","Join Logic"],"tags":[]},{"location":"concepts/join-logic/#max_lookback","level":3,"title":"max_lookback","text":"<p>Maximum age of a feature value. Default: <code>\"365d\"</code>. Features older than this are treated as missing.</p> <pre><code>result = timefence.build(\n    labels=labels,\n    features=features,\n    max_lookback=\"90d\",  # Only use features from last 90 days\n)\n</code></pre>","path":["Core Concepts","Join Logic"],"tags":[]},{"location":"concepts/join-logic/#max_staleness","level":3,"title":"max_staleness","text":"<p>If set, features older than this threshold are treated as missing even if within the lookback window. Must satisfy: <code>max_staleness &gt; embargo</code>.</p> <pre><code>result = timefence.build(\n    labels=labels,\n    features=features,\n    max_staleness=\"30d\",  # Drop features older than 30 days\n)\n</code></pre>","path":["Core Concepts","Join Logic"],"tags":[]},{"location":"concepts/join-logic/#on_missing","level":3,"title":"on_missing","text":"<p>What to do when no valid feature value exists for a label row:</p> Value Behavior <code>\"null\"</code> Keep the row, fill feature columns with NULL (default) <code>\"skip\"</code> Drop the row entirely","path":["Core Concepts","Join Logic"],"tags":[]},{"location":"concepts/join-logic/#inspecting-the-join-plan","level":2,"title":"Inspecting the join plan","text":"<p>Preview what SQL will be generated without executing:</p> <pre><code>timefence explain --labels data/labels.parquet --features features.py\n</code></pre> <pre><code>JOIN PLAN for 5,000 label rows\n\nFor each label row (keys, label_time):\n\n  1. rolling_spend_30d\n     Source:  data/transactions.parquet\n     Join:    feature_time &lt; label_time - INTERVAL '1d'\n     Window:  [label_time - 365d, label_time - 1d)\n     Embargo: 1d\n     Strategy: row_number\n\n  2. country\n     Source:  data/users.parquet\n     Join:    feature_time &lt; label_time\n     Window:  [label_time - 365d, label_time)\n     Embargo: none\n     Strategy: asof\n</code></pre> <p>Or in Python:</p> <pre><code>result = timefence.explain(labels=labels, features=[spend, country])\nprint(result)        # Same formatted output\nprint(result.plan)   # Raw list of dicts per feature\n</code></pre> <p>See the explain() API reference for full details on the <code>ExplainResult</code> object.</p>","path":["Core Concepts","Join Logic"],"tags":[]},{"location":"concepts/splits/","level":1,"title":"Time-Based Splits","text":"<p>Build separate train/validation/test files split by label time.</p>","path":["Core Concepts","Time-Based Splits"],"tags":[]},{"location":"concepts/splits/#usage","level":2,"title":"Usage","text":"<pre><code>result = timefence.build(\n    labels=labels,\n    features=[rolling_spend, user_country],\n    output=\"dataset.parquet\",\n    splits={\n        \"train\": (\"2023-01-01\", \"2024-01-01\"),\n        \"valid\": (\"2024-01-01\", \"2024-07-01\"),\n        \"test\":  (\"2024-07-01\", \"2025-01-01\"),\n    },\n)\n\n# result.splits = {\"train\": Path(...), \"valid\": Path(...), \"test\": Path(...)}\n</code></pre> <p>Each split file contains only labels whose <code>label_time</code> falls within the given range. All temporal correctness guarantees still apply per-row.</p>","path":["Core Concepts","Time-Based Splits"],"tags":[]},{"location":"concepts/splits/#cli","level":2,"title":"CLI","text":"<pre><code>timefence build \\\n  --labels data/labels.parquet \\\n  --features features.py \\\n  -o train.parquet \\\n  --split train:2023-01-01:2024-01-01 \\\n  --split test:2024-01-01:2025-01-01\n</code></pre>","path":["Core Concepts","Time-Based Splits"],"tags":[]},{"location":"concepts/splits/#why-time-based-splits","level":2,"title":"Why time-based splits?","text":"<p>Random splits in time-series data cause leakage: the model sees future patterns in the training set. Time-based splits ensure the model is always evaluated on data it has never seen from the future.</p>","path":["Core Concepts","Time-Based Splits"],"tags":[]},{"location":"concepts/temporal-correctness/","level":1,"title":"Temporal Correctness","text":"<p>The core invariant Timefence enforces:</p> <p>The Rule</p> <pre><code>feature_time &lt; label_time - embargo\n</code></pre> <p>For every row in your training set, the feature value used must have been available strictly before the label event time (minus any embargo buffer). This prevents future data leakage — the most common and hardest-to-detect source of inflated ML metrics.</p>","path":["Core Concepts","Temporal Correctness"],"tags":[]},{"location":"concepts/temporal-correctness/#why-it-matters","level":2,"title":"Why it matters","text":"<p>When you join features to labels with a <code>LEFT JOIN</code> or <code>merge_asof</code>, each label gets the latest feature row — including data from after the event you're predicting. The model trains on the future. Offline metrics look great. Production doesn't match.</p> <p>No error, no warning, no way to tell from the output alone.</p>","path":["Core Concepts","Temporal Correctness"],"tags":[]},{"location":"concepts/temporal-correctness/#strict-vs-inclusive","level":2,"title":"Strict vs Inclusive","text":"<p>In <code>inclusive</code> join mode, the condition relaxes to:</p> <pre><code>feature_time &lt;= label_time - embargo\n</code></pre> Mode Condition Use When <code>\"strict\"</code> <code>feature_time &lt; label_time</code> Default. No same-timestamp leakage. <code>\"inclusive\"</code> <code>feature_time &lt;= label_time</code> When same-timestamp is safe (e.g., static attributes).","path":["Core Concepts","Temporal Correctness"],"tags":[]},{"location":"concepts/temporal-correctness/#how-timefence-enforces-it","level":2,"title":"How Timefence enforces it","text":"<p>Timefence generates SQL (ASOF JOIN or ROW_NUMBER) and runs it in an embedded DuckDB. For each label row, it finds the most recent feature value that satisfies the temporal constraint. Every query is inspectable via <code>timefence -v build</code> or <code>timefence explain</code>.</p>","path":["Core Concepts","Temporal Correctness"],"tags":[]},{"location":"getting-started/installation/","level":1,"title":"Installation","text":"","path":["Getting Started","Installation"],"tags":[]},{"location":"getting-started/installation/#requirements","level":2,"title":"Requirements","text":"<ul> <li>Python 3.9+</li> <li>pip or uv</li> </ul>","path":["Getting Started","Installation"],"tags":[]},{"location":"getting-started/installation/#install","level":2,"title":"Install","text":"<pre><code>pip install timefence\n</code></pre>","path":["Getting Started","Installation"],"tags":[]},{"location":"getting-started/installation/#optional-extras","level":3,"title":"Optional extras","text":"<pre><code># Jupyter notebook support\npip install timefence[notebook]\n\n# Development dependencies\npip install timefence[dev]\n</code></pre> <p>dbt integration</p> <p>A <code>timefence[dbt]</code> extra is planned for a future release. The <code>from_dbt()</code> function exists but is not yet implemented.</p>","path":["Getting Started","Installation"],"tags":[]},{"location":"getting-started/installation/#dependencies","level":2,"title":"Dependencies","text":"<p>Timefence depends on:</p> Package Version Purpose <code>duckdb</code> &gt;= 1.0.0 Columnar SQL engine <code>click</code> &gt;= 8.0.0 CLI framework <code>rich</code> &gt;= 13.0.0 Terminal output <code>pandas</code> &gt;= 1.5.0 DataFrame interop <code>pyyaml</code> &gt;= 6.0 YAML config <p>No Spark, no JVM, no cloud infrastructure.</p>","path":["Getting Started","Installation"],"tags":[]},{"location":"getting-started/installation/#performance","level":2,"title":"Performance","text":"<p>Built on DuckDB's columnar engine. Median of 3 runs after warmup (Intel i7, 16 GB):</p> Scenario Labels Features Build Audit Small project 100K 1 0.5s 0.3s Typical project 100K 10 1.9s 1.7s Large project 1M 1 3.0s 2.0s Large + many features 1M 10 12s 8.5s <p>Adding embargo, staleness, and splits costs seconds, not minutes.</p> Run benchmarks yourself <pre><code>uv run python benchmarks/bench.py --quick\nuv run python benchmarks/bench.py --quick --include-pandas\n</code></pre>","path":["Getting Started","Installation"],"tags":[]},{"location":"getting-started/installation/#verify-installation","level":2,"title":"Verify installation","text":"<pre><code>timefence --version\ntimefence doctor\n</code></pre>","path":["Getting Started","Installation"],"tags":[]},{"location":"getting-started/quickstart/","level":1,"title":"Quickstart","text":"<p>Get up and running in 60 seconds.</p>","path":["Getting Started","Quickstart"],"tags":[]},{"location":"getting-started/quickstart/#generate-a-sample-project","level":2,"title":"Generate a sample project","text":"<pre><code>timefence quickstart churn-example\ncd churn-example\n</code></pre> <p>This creates a self-contained directory with:</p> <ul> <li><code>timefence.yaml</code> — Project configuration</li> <li><code>features.py</code> — 4 feature definitions</li> <li><code>data/</code> — Synthetic parquet files (users, transactions, labels)</li> <li><code>data/train_LEAKY.parquet</code> — Pre-built dataset with planted leakage</li> <li><code>README.md</code> — Next-step instructions</li> </ul>","path":["Getting Started","Quickstart"],"tags":[]},{"location":"getting-started/quickstart/#audit-the-leaky-dataset","level":2,"title":"Audit the leaky dataset","text":"<pre><code>timefence audit data/train_LEAKY.parquet\n</code></pre> <pre><code>TEMPORAL AUDIT REPORT\nScanned 5,000 rows\n\nWARNING  LEAKAGE DETECTED in 3 of 4 features\n\n  LEAK  rolling_spend_30d\n        1,520 rows (30.4%) use feature data from the future\n        Severity: HIGH\n\n  LEAK  days_since_login\n        4,909 rows (98.2%) use feature data from the future\n        Severity: HIGH\n\n  OK    user_country - clean (5,000 rows)\n  OK    account_age_days - clean (5,000 rows)\n</code></pre>","path":["Getting Started","Quickstart"],"tags":[]},{"location":"getting-started/quickstart/#build-a-clean-dataset","level":2,"title":"Build a clean dataset","text":"<pre><code>timefence build -o train_CLEAN.parquet\n</code></pre> <pre><code>Building training set...\n\n  Labels     5,000 rows from data/labels.parquet\n  Features   4 features\n\n  Joining with point-in-time correctness (feature_time &lt; label_time):\n\n  OK  user_country         5,000 / 5,000 matched\n  OK  account_age_days     5,000 / 5,000 matched\n  OK  rolling_spend_30d    5,000 / 5,000 matched\n  OK  days_since_login     5,000 / 5,000 matched\n\n  Written   train_CLEAN.parquet (5,000 rows, 7 cols)\n</code></pre>","path":["Getting Started","Quickstart"],"tags":[]},{"location":"getting-started/quickstart/#verify-its-clean","level":2,"title":"Verify it's clean","text":"<pre><code>timefence audit train_CLEAN.parquet\n# ALL CLEAN - no temporal leakage detected\n</code></pre>","path":["Getting Started","Quickstart"],"tags":[]},{"location":"getting-started/quickstart/#next-steps","level":2,"title":"Next steps","text":"<ul> <li>Audit your own data without changing your pipeline</li> <li>Build from scratch with Python API</li> <li>Add to CI to prevent leakage in production</li> </ul>","path":["Getting Started","Quickstart"],"tags":[]},{"location":"guides/audit/","level":1,"title":"Guide: Audit Existing Data","text":"<p>Already have a training dataset? Audit it for temporal leakage without rebuilding.</p>","path":["Guides","Guide: Audit Existing Data"],"tags":[]},{"location":"guides/audit/#step-1-inspect-your-data","level":2,"title":"Step 1: Inspect your data","text":"<pre><code>timefence inspect data/train.parquet\n</code></pre> <p>This shows column types and suggests which columns are likely keys and timestamps.</p>","path":["Guides","Guide: Audit Existing Data"],"tags":[]},{"location":"guides/audit/#step-2-run-the-audit","level":2,"title":"Step 2: Run the audit","text":"PythonCLI <pre><code>import timefence\n\ntransactions = timefence.Source(\n    path=\"data/transactions.parquet\",\n    keys=[\"user_id\"],\n    timestamp=\"created_at\",\n)\n\nrolling_spend = timefence.Feature(\n    source=transactions,\n    columns=[\"amount\"],\n    embargo=\"1d\",\n)\n\nreport = timefence.audit(\n    data=\"data/train.parquet\",\n    features=[rolling_spend],\n    keys=[\"user_id\"],\n    label_time=\"label_time\",\n)\n\nprint(report)\n</code></pre> <pre><code>timefence audit data/train.parquet \\\n  --features features.py \\\n  --keys user_id \\\n  --label-time label_time\n</code></pre>","path":["Guides","Guide: Audit Existing Data"],"tags":[]},{"location":"guides/audit/#step-3-inspect-results","level":2,"title":"Step 3: Inspect results","text":"<pre><code>import timefence\n\n# (assuming `report` from step 2)\n\n# Check overall status\nreport.has_leakage        # True/False\nreport.leaky_features     # [\"rolling_spend_30d\"]\nreport.clean_features     # [\"user_country\"]\n\n# Per-feature details\ndetail = report[\"rolling_spend_30d\"]\ndetail.leaky_row_count    # 1520\ndetail.leaky_row_pct      # 0.304\ndetail.severity           # \"HIGH\"\ndetail.max_leakage        # timedelta(days=15)\n\n# Export\nreport.to_html(\"audit_report.html\")\nreport.to_json(\"audit_report.json\")\n</code></pre>","path":["Guides","Guide: Audit Existing Data"],"tags":[]},{"location":"guides/audit/#step-4-assert-in-tests","level":2,"title":"Step 4: Assert in tests","text":"<pre><code>report.assert_clean()  # Raises TimefenceLeakageError if leakage found\n</code></pre> <p>See the audit() API reference for full parameter documentation.</p>","path":["Guides","Guide: Audit Existing Data"],"tags":[]},{"location":"guides/build/","level":1,"title":"Guide: Build from Scratch","text":"<p>Build a point-in-time correct training dataset from raw data.</p>","path":["Guides","Guide: Build from Scratch"],"tags":[]},{"location":"guides/build/#full-example","level":2,"title":"Full example","text":"<pre><code>import timefence\n\n# 1. Define sources\ntransactions = timefence.Source(\n    path=\"data/transactions.parquet\",\n    keys=[\"user_id\"],\n    timestamp=\"created_at\",\n)\n\nusers = timefence.Source(\n    path=\"data/users.parquet\",\n    keys=[\"user_id\"],\n    timestamp=\"updated_at\",\n)\n\n# 2. Define features\nrolling_spend = timefence.Feature(\n    source=transactions,\n    sql=\"\"\"\n        SELECT user_id, created_at AS feature_time,\n        SUM(amount) OVER (\n            PARTITION BY user_id\n            ORDER BY created_at\n            RANGE INTERVAL 30 DAYS PRECEDING\n        ) AS spend_30d\n        FROM {source}\n    \"\"\",\n    name=\"rolling_spend_30d\",\n    embargo=\"1d\",\n)\n\nuser_country = timefence.Feature(\n    source=users,\n    columns=[\"country\"],\n)\n\n# 3. Define labels\nlabels = timefence.Labels(\n    path=\"data/labels.parquet\",\n    keys=[\"user_id\"],\n    label_time=\"label_time\",\n    target=\"churned\",\n)\n\n# 4. Build\nresult = timefence.build(\n    labels=labels,\n    features=[rolling_spend, user_country],\n    output=\"train_CLEAN.parquet\",\n)\n\nprint(result)\n# rows: 5000, columns: 4, duration: 0.8s\n</code></pre>","path":["Guides","Guide: Build from Scratch"],"tags":[]},{"location":"guides/build/#feature-modes","level":2,"title":"Feature modes","text":"<p>Timefence supports three ways to define features:</p> Column SelectionSQLPython Transform <pre><code>country = timefence.Feature(source=users, columns=[\"country\"])\n</code></pre> <pre><code>spend = timefence.Feature(\n    source=transactions,\n    sql=\"\"\"\n        SELECT user_id, created_at AS feature_time,\n        SUM(amount) OVER (...) AS spend_30d\n        FROM {source}\n    \"\"\",\n    name=\"rolling_spend_30d\",\n    embargo=\"1d\",\n)\n</code></pre> <pre><code>def compute_score(conn, source_table):\n    return conn.sql(f\"\"\"\n        SELECT user_id, created_at AS feature_time,\n               raw_score * 2.5 AS adjusted_score\n        FROM {source_table}\n    \"\"\")\n\nscore = timefence.Feature(source=transactions, transform=compute_score)\n</code></pre>","path":["Guides","Guide: Build from Scratch"],"tags":[]},{"location":"guides/build/#cli-equivalent","level":2,"title":"CLI equivalent","text":"<pre><code>timefence build \\\n  --labels data/labels.parquet \\\n  --features features.py \\\n  -o train.parquet\n</code></pre> <p>See the build() API reference for full parameter documentation.</p>","path":["Guides","Guide: Build from Scratch"],"tags":[]},{"location":"guides/ci/","level":1,"title":"Guide: CI/CD Integration","text":"<p>Stop leakage before it reaches production.</p>","path":["Guides","Guide: CI/CD Integration"],"tags":[]},{"location":"guides/ci/#cli","level":2,"title":"CLI","text":"<pre><code># Exits with code 1 if leakage is found\ntimefence audit data/train.parquet --strict\n</code></pre>","path":["Guides","Guide: CI/CD Integration"],"tags":[]},{"location":"guides/ci/#python","level":2,"title":"Python","text":"<pre><code>report = timefence.audit(\n    data=\"data/train.parquet\",\n    features=[rolling_spend, user_country],\n    keys=[\"user_id\"],\n    label_time=\"label_time\",\n)\n\n# Raises TimefenceLeakageError if any leakage detected\nreport.assert_clean()\n</code></pre>","path":["Guides","Guide: CI/CD Integration"],"tags":[]},{"location":"guides/ci/#github-actions","level":2,"title":"GitHub Actions","text":"<pre><code>- name: Audit training data\n  run: |\n    pip install timefence\n    timefence audit data/train.parquet \\\n      --features features.py \\\n      --keys user_id \\\n      --label-time label_time \\\n      --strict\n</code></pre>","path":["Guides","Guide: CI/CD Integration"],"tags":[]},{"location":"guides/ci/#gitlab-ci","level":2,"title":"GitLab CI","text":"<pre><code>audit:\n  image: python:3.12\n  script:\n    - pip install timefence\n    - timefence audit data/train.parquet --features features.py --strict\n</code></pre>","path":["Guides","Guide: CI/CD Integration"],"tags":[]},{"location":"guides/ci/#what-strict-does","level":2,"title":"What <code>--strict</code> does","text":"<ul> <li>Exit code <code>0</code> = no leakage detected (pipeline continues)</li> <li>Exit code <code>1</code> = leakage detected (pipeline fails)</li> </ul> <p>Combine with <code>--json</code> for machine-readable output:</p> <pre><code>timefence audit data/train.parquet --strict --json &gt; audit-result.json\n</code></pre> <p>Or generate an HTML report for review:</p> <pre><code>timefence audit data/train.parquet --html audit-report.html\n</code></pre>","path":["Guides","Guide: CI/CD Integration"],"tags":[]},{"location":"reference/configuration/","level":1,"title":"Configuration","text":"<p>Timefence looks for a <code>timefence.yaml</code> (or <code>timefence.yml</code>) in the current directory. All fields are optional — CLI flags and Python API arguments take precedence.</p>","path":["Reference","Configuration"],"tags":[]},{"location":"reference/configuration/#full-example","level":2,"title":"Full example","text":"<pre><code>name: churn-model\nversion: \"1.0\"\n\n# Feature file(s)\nfeatures:\n  - features.py\n\n# Label configuration\nlabels:\n  path: data/labels.parquet\n  keys: [user_id]\n  label_time: label_time\n  target: [churned]\n\n# Default parameters\ndefaults:\n  max_lookback: 365d\n  max_staleness: null    # or e.g. \"30d\"\n  join: strict          # \"strict\" or \"inclusive\"\n  on_missing: \"null\"    # \"null\" or \"skip\"\n\n# Store directory for build tracking\nstore: .timefence/\n\n# Output directory (relative paths in build resolve against this)\noutput:\n  dir: artifacts/\n</code></pre> <p>Precedence</p> <p>CLI flags &gt; Python API arguments &gt; <code>timefence.yaml</code> &gt; built-in defaults.</p>","path":["Reference","Configuration"],"tags":[]},{"location":"reference/durations/","level":1,"title":"Duration Format","text":"<p>Timefence accepts human-readable duration strings wherever a time duration is expected (<code>embargo</code>, <code>max_lookback</code>, <code>max_staleness</code>).</p>","path":["Reference","Duration Format"],"tags":[]},{"location":"reference/durations/#formats","level":2,"title":"Formats","text":"Format Example Meaning <code>Nd</code> <code>\"30d\"</code> 30 days <code>Nh</code> <code>\"6h\"</code> 6 hours <code>Nm</code> <code>\"30m\"</code> 30 minutes <code>Ns</code> <code>\"15s\"</code> 15 seconds Combined <code>\"1d12h\"</code> 1 day and 12 hours Zero <code>\"0d\"</code> or <code>\"0\"</code> No duration","path":["Reference","Duration Format"],"tags":[]},{"location":"reference/durations/#python-api","level":2,"title":"Python API","text":"<p>You can also pass a Python <code>timedelta</code> object directly:</p> <pre><code>from datetime import timedelta\n\nfeature = timefence.Feature(\n    source=transactions,\n    columns=[\"amount\"],\n    embargo=timedelta(days=1),\n)\n\nresult = timefence.build(\n    labels=labels,\n    features=[feature],\n    max_lookback=timedelta(days=365),\n)\n</code></pre>","path":["Reference","Duration Format"],"tags":[]},{"location":"reference/errors/","level":1,"title":"Errors","text":"<p>All Timefence errors inherit from <code>TimefenceError</code> and follow a consistent format:</p> <ul> <li>WHAT happened</li> <li>WHY it matters</li> <li>WHERE in the data</li> <li>HOW TO FIX it</li> </ul>","path":["Reference","Errors"],"tags":[]},{"location":"reference/errors/#error-hierarchy","level":2,"title":"Error hierarchy","text":"<pre><code>TimefenceError (base)\n├── TimefenceSchemaError\n├── TimefenceDuplicateError\n├── TimefenceTimezoneError\n├── TimefenceConfigError\n├── TimefenceLeakageError\n└── TimefenceValidationError\n</code></pre>","path":["Reference","Errors"],"tags":[]},{"location":"reference/errors/#timefenceschemaerror","level":2,"title":"TimefenceSchemaError","text":"<p>Raised when expected columns are missing or types are mismatched between source, labels, and feature definitions.</p> <pre><code>TimefenceSchemaError: Feature 'rolling_spend' is missing required key column(s): ['user_id'].\n\n  Point-in-time joins require matching keys between labels and features.\n  Without ['user_id'], Timefence can't determine which feature rows belong to which entity.\n\n  Expected keys: ['user_id']\n  Actual columns: ['customer_id', 'created_at', 'amount']\n  'customer_id' is similar to 'user_id' — possible rename?\n\n  Fix: Add key_mapping to your feature definition:\n    key_mapping={'user_id': 'customer_id'}\n</code></pre> <p>Common causes: Column renamed between sources, typos in key names, using the wrong file.</p> <p>Fix: Correct the column name, or use <code>key_mapping</code> on the Feature to map label keys to source keys.</p>","path":["Reference","Errors"],"tags":[]},{"location":"reference/errors/#timefenceduplicateerror","level":2,"title":"TimefenceDuplicateError","text":"<p>Raised when duplicate <code>(key, timestamp)</code> pairs exist in a source and <code>on_duplicate=\"error\"</code> (default).</p> <pre><code>TimefenceDuplicateError: Feature 'user_country' has 42 duplicate (key, feature_time) pairs.\n\n  When multiple feature rows have the same key and timestamp, the\n  point-in-time join becomes non-deterministic. Timefence cannot guarantee\n  which row would be selected.\n\n  Example duplicates (showing first 3):\n    {'\"user_id\"': 101, '\"updated_at\"': Timestamp('2024-03-15 10:00:00'), 'cnt': 3}\n    {'\"user_id\"': 205, '\"updated_at\"': Timestamp('2024-03-16 14:30:00'), 'cnt': 2}\n\n  Fix (pick one):\n    1. Deduplicate in your source data or SQL\n    2. Set: timefence.Feature(..., on_duplicate=\"keep_any\")\n</code></pre> <p>Common causes: Multiple events at the same timestamp, duplicated source data, ETL issues.</p> <p>Fix: Deduplicate your source, or set <code>on_duplicate=\"keep_any\"</code> if you don't care which row is selected.</p>","path":["Reference","Errors"],"tags":[]},{"location":"reference/errors/#timefencetimezoneerror","level":2,"title":"TimefenceTimezoneError","text":"<p>Raised when mixing timezone-aware and timezone-naive timestamps across sources and labels.</p> <pre><code>TimefenceTimezoneError: Mixed timezones between labels and feature 'rolling_spend'.\n\n  Labels 'label_time' is timezone-aware (UTC).\n  Feature 'rolling_spend' timestamp is timezone-naive.\n\n  Comparing these directly could shift joins by hours.\n\n  Sample values:\n    label_time:   2024-03-15 10:00:00+00:00\n    feature_time: 2024-03-15 10:00:00\n</code></pre> <p>Common causes: One data source uses UTC-aware timestamps, another uses naive timestamps.</p> <p>Fix: Normalize all timestamps to the same type — either all timezone-aware or all timezone-naive.</p>","path":["Reference","Errors"],"tags":[]},{"location":"reference/errors/#timefenceconfigerror","level":2,"title":"TimefenceConfigError","text":"<p>Raised for invalid parameter combinations.</p> <pre><code>TimefenceConfigError: embargo (400d) must be less than max_lookback (365d).\n\n  When embargo equals or exceeds max_lookback, the join window is empty —\n  no feature can ever match. This is almost certainly a misconfiguration.\n\n  Current: max_lookback=365d, embargo=400d → empty window\n  Likely intent: max_lookback=365d, embargo=1d\n\n  Fix: Increase max_lookback or decrease embargo.\n</code></pre> <p>Other examples:</p> <ul> <li><code>join</code> must be <code>\"strict\"</code> or <code>\"inclusive\"</code></li> <li><code>on_missing</code> must be <code>\"null\"</code> or <code>\"skip\"</code></li> <li><code>on_duplicate</code> must be <code>\"error\"</code> or <code>\"keep_any\"</code></li> <li>Feature requires exactly one of <code>columns</code>, <code>sql</code>, or <code>transform</code></li> <li>Duplicate or colliding feature names</li> </ul>","path":["Reference","Errors"],"tags":[]},{"location":"reference/errors/#timefenceleakageerror","level":2,"title":"TimefenceLeakageError","text":"<p>Raised by <code>report.assert_clean()</code> when temporal leakage is detected.</p> <pre><code>TimefenceLeakageError: Temporal leakage detected in features: rolling_spend_30d, days_since_login\n</code></pre> <p>Where it appears: Only from <code>AuditReport.assert_clean()</code>. The <code>audit()</code> function itself returns a report — it does not raise.</p>","path":["Reference","Errors"],"tags":[]},{"location":"reference/errors/#timefencevalidationerror","level":2,"title":"TimefenceValidationError","text":"<p>Raised for general input validation failures.</p> <pre><code>TimefenceValidationError: Source requires either 'path' or 'df' parameter.\nTimefenceValidationError: Source accepts either 'path' or 'df', not both.\nTimefenceValidationError: Labels 'keys' cannot be empty. Provide at least one entity key column.\n</code></pre>","path":["Reference","Errors"],"tags":[]},{"location":"reference/errors/#handling-errors","level":2,"title":"Handling errors","text":"<pre><code>import timefence\nfrom timefence.errors import (\n    TimefenceError,\n    TimefenceSchemaError,\n    TimefenceLeakageError,\n)\n\n# Catch a specific error\ntry:\n    result = timefence.build(labels=labels, features=[feature], output=\"train.parquet\")\nexcept TimefenceSchemaError as e:\n    print(f\"Schema issue: {e}\")\n    # Inspect the error message for suggested fixes\n\n# Catch any Timefence error\ntry:\n    result = timefence.build(labels=labels, features=[feature], output=\"train.parquet\")\nexcept TimefenceError as e:\n    print(f\"Timefence error: {e}\")\n\n# CI pattern: assert_clean raises TimefenceLeakageError\ntry:\n    report = timefence.audit(data=\"train.parquet\", features=[feature], keys=[\"user_id\"], label_time=\"label_time\")\n    report.assert_clean()\nexcept TimefenceLeakageError:\n    print(\"Leakage found — failing build\")\n    sys.exit(1)\n</code></pre>","path":["Reference","Errors"],"tags":[]},{"location":"reference/troubleshooting/","level":1,"title":"Troubleshooting","text":"<p>Common issues and how to fix them.</p>","path":["Reference","Troubleshooting"],"tags":[]},{"location":"reference/troubleshooting/#feature-x-is-missing-required-key-column","level":2,"title":"\"Feature X is missing required key column\"","text":"<p>Error: <code>TimefenceSchemaError</code></p> <p>Your feature source uses different column names than your labels. For example, labels have <code>user_id</code> but the source has <code>customer_id</code>.</p> <p>Fix: Add <code>key_mapping</code> to the feature:</p> <pre><code>feature = timefence.Feature(\n    source=transactions,\n    columns=[\"amount\"],\n    key_mapping={\"user_id\": \"customer_id\"},\n)\n</code></pre>","path":["Reference","Troubleshooting"],"tags":[]},{"location":"reference/troubleshooting/#duplicate-key-feature_time-pairs","level":2,"title":"\"Duplicate (key, feature_time) pairs\"","text":"<p>Error: <code>TimefenceDuplicateError</code></p> <p>Your source has multiple rows with the same key and timestamp. The point-in-time join can't determine which row to select.</p> <p>Fix (pick one):</p> <ol> <li>Deduplicate your source data upstream</li> <li>Accept non-determinism: <code>timefence.Feature(..., on_duplicate=\"keep_any\")</code></li> </ol>","path":["Reference","Troubleshooting"],"tags":[]},{"location":"reference/troubleshooting/#mixed-timezones-between-labels-and-feature","level":2,"title":"\"Mixed timezones between labels and feature\"","text":"<p>Error: <code>TimefenceTimezoneError</code></p> <p>One timestamp is timezone-aware (e.g., <code>2024-01-01 10:00:00+00:00</code>) and the other is naive (e.g., <code>2024-01-01 10:00:00</code>). Comparing them directly could shift joins by hours.</p> <p>Fix: Normalize all timestamps to the same type before passing to Timefence.</p>","path":["Reference","Troubleshooting"],"tags":[]},{"location":"reference/troubleshooting/#embargo-must-be-less-than-max_lookback","level":2,"title":"\"embargo must be less than max_lookback\"","text":"<p>Error: <code>TimefenceConfigError</code></p> <p>Your embargo is larger than the lookback window, making it impossible for any feature to match.</p> <p>Fix: Either increase <code>max_lookback</code> or decrease <code>embargo</code>:</p> <pre><code>result = timefence.build(\n    labels=labels,\n    features=[feature],\n    output=\"train.parquet\",\n    max_lookback=\"730d\",  # Increase from default 365d\n)\n</code></pre>","path":["Reference","Troubleshooting"],"tags":[]},{"location":"reference/troubleshooting/#audit-says-leakage-detected-what-now","level":2,"title":"Audit says \"LEAKAGE DETECTED\" — what now?","text":"<p>This means your existing training data has rows where <code>feature_time &gt;= label_time</code> (in the default strict mode). In inclusive mode (<code>join=\"inclusive\"</code>), leakage is <code>feature_time &gt; label_time</code>. Either way, the feature was computed after the event you're predicting — meaning your model trained on the future.</p> <p>Options:</p> <ol> <li>Rebuild the dataset with <code>timefence build</code> to get temporally correct data</li> <li>Investigate which features leak and by how much (check <code>report[\"feature_name\"].severity</code>)</li> <li>Add to CI with <code>--strict</code> to prevent it from happening again</li> </ol>","path":["Reference","Troubleshooting"],"tags":[]},{"location":"reference/troubleshooting/#build-is-slow","level":2,"title":"Build is slow","text":"<p>Timefence processes data through DuckDB's columnar engine. If builds are slow:</p> <ol> <li>Check data size: How many label rows × how many features? See benchmarks</li> <li>Enable caching: Pass a <code>Store</code> to avoid recomputing unchanged features</li> <li>Use Parquet over CSV: Parquet is significantly faster due to columnar reads</li> <li>Check feature SQL complexity: Complex window functions take longer — consider pre-computing</li> </ol>","path":["Reference","Troubleshooting"],"tags":[]},{"location":"reference/troubleshooting/#cannot-auto-detect-format","level":2,"title":"\"Cannot auto-detect format\"","text":"<p>Error: <code>TimefenceValidationError</code></p> <p>The file extension isn't <code>.parquet</code>, <code>.pq</code>, or <code>.csv</code>.</p> <p>Fix: Specify the format explicitly:</p> <pre><code>source = timefence.Source(\n    path=\"data/file.dat\",\n    keys=[\"user_id\"],\n    timestamp=\"ts\",\n    format=\"parquet\",  # or \"csv\"\n)\n</code></pre>","path":["Reference","Troubleshooting"],"tags":[]},{"location":"reference/troubleshooting/#feature-requires-exactly-one-of-columns-sql-or-transform","level":2,"title":"\"Feature requires exactly one of columns, sql, or transform\"","text":"<p>Error: <code>TimefenceConfigError</code></p> <p>You passed zero or more than one of <code>columns</code>, <code>sql</code>, or <code>transform</code> to a Feature.</p> <p>Fix: Use exactly one:</p> <pre><code># Column mode\ntimefence.Feature(source=src, columns=[\"country\"])\n\n# SQL mode\ntimefence.Feature(source=src, sql=\"SELECT ...\", name=\"my_feature\")\n\n# Transform mode\ntimefence.Feature(source=src, transform=my_function)\n</code></pre>","path":["Reference","Troubleshooting"],"tags":[]},{"location":"reference/troubleshooting/#asof-join-fallback-warning","level":2,"title":"ASOF JOIN fallback warning","text":"<p>When the log shows \"ASOF JOIN failed, falling back to ROW_NUMBER,\" this means DuckDB's ASOF JOIN couldn't handle the query (uncommon). Timefence automatically retried with the ROW_NUMBER strategy. The result is still correct — ROW_NUMBER is the universal fallback.</p> <p>No action needed. To see why the fallback happened, run with <code>--debug</code>:</p> <pre><code>timefence --debug build --labels data/labels.parquet --features features.py -o train.parquet\n</code></pre>","path":["Reference","Troubleshooting"],"tags":[]},{"location":"reference/troubleshooting/#still-stuck","level":2,"title":"Still stuck?","text":"<ol> <li>Run <code>timefence doctor</code> to check your project setup</li> <li>Run <code>timefence inspect data/your_file.parquet</code> to inspect columns and types</li> <li>Open an issue at github.com/gauthierpiarrette/timefence</li> </ol>","path":["Reference","Troubleshooting"],"tags":[]}]}